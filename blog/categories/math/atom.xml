<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Bobbie's Blog]]></title>
  <link href="http://bobbielf2.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://bobbielf2.github.io/"/>
  <updated>2021-05-24T00:05:24-04:00</updated>
  <id>http://bobbielf2.github.io/</id>
  <author>
    <name><![CDATA[Bowei "Bobbie" Wu .]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Zeta Function = Sum - Integral]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/05/23/zeta-function-equals-sum-integral/"/>
    <updated>2021-05-23T22:57:33-04:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/05/23/zeta-function-equals-sum-integral</id>
    <content type="html"><![CDATA[<p>Recently I saw some memes about the peculiar “sum”</p>

<script type="math/tex; mode=display">1+2+3+4+\dots = -1/12</script>

<p><img class="center" src="/images/blog_figures/zeta_simpsons.jpeg" width="400"></p>

<p><img class="center" src="/images/blog_figures/zeta_trolley_problem.jpeg" width="400"></p>

<p>These memes are inspired by the Riemann zeta function</p>

<script type="math/tex; mode=display">\zeta(s) := \sum_{n=1}^\infty\frac{1}{n^s},\qquad (s>1)</script>

<p>in which directly substituting $s=-1$ gives the $-\tfrac{1}{12}$ “sum” above.</p>

<!--more-->

<p>This $1+2+3+4+\dots = -1/12$ joke was first popularized by the 2014 <a href="https://youtu.be/w-I6XTVZXww">numberphile video</a>. The video went viral and caused a lot of confusions due to the lack of proper explanations.</p>

<p>Indeed, adding $1+2+3+4+\dots$ will blow up to infinity in the usual sense. The definition of $\zeta(s)$ by summing $1/n^s$ only makes sense for $s&gt;1$; when $s&lt;1$, the value $\zeta(s)$ is in fact obtained by <a href="https://en.wikipedia.org/wiki/Analytic_continuation">analytic continuation</a>. Later in 2018, Mathologer gave an <a href="https://youtu.be/YuIIjLr6vUA">accessible clarification</a> which only requires a minimal background in calculus to understand.</p>

<h3 id="a-new-perspective-on-the-zeta-function">A New Perspective on the Zeta Function</h3>

<p>Here, I will provide another interesting perspective on the continuation of $\zeta(s)$, which I think is quite beautiful and deserves more attentions. This is stated in the title of this article:</p>

<blockquote>
  <p>zeta function $=$ sum $-$ integral.</p>
</blockquote>

<p>Specifically,</p>

<script type="math/tex; mode=display">\zeta(s) = \sum_{n=1}^\infty\frac{1}{n^s} - \int_0^\infty\frac{1}{x^s}\,dx.</script>

<p>This relationship holds for any $s$ in the complex plane $\mathbb{C}$, provided we define this expression appropriately.</p>

<h3 id="the-proof">The Proof</h3>

<p>For simplicity, let’s start with $-1&lt;\mathrm{Re}(s)&lt;1$, which is a vertical strip in $\mathbb{C}$ where both the sum and the integral diverge. Then an appropriate zeta-sum-integral connection can be written as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\zeta(s) = \lim_{N\to\infty}\Big(\sum_{n=1}^N\frac{1}{n^s} - \int_0^{N+\frac{1}{2}}\frac{1}{x^s}\,dx\Big),\qquad -1<\mathrm{Re}(s)<1 %]]&gt;</script>

<p>To paint a clearer picture, let’s use some visual help and consider the above limit on the real line:</p>

<p><img class="center" src="/images/blog_figures/zeta_proof_visual.png" width="500"></p>

<p>Here, the red dots are the integer points $n = 1, 2, 3,\dots$, and the blue bars divide the real line into boxes of unit length $[\frac12, \frac32], [\frac32, \frac{5}{2}], [\frac{5}{2}, \frac{7}{2}], \dots$</p>

<p>Based on this picture, let’s first define some quantities of interest:</p>

<ul>
  <li>Define the partial sum $S_N(s) := \sum_{n=1}^N\frac{1}{n^s}$	which is a sum on the first $N$ red dots in the picture above.</li>
  <li>Define the finite integral $I_N(s) := \int_{\frac12}^{N+\frac12}\frac{1}{x^s}\,dx$	which is an integral in the first $N$ boxes defined by the blue bars in the picture.</li>
  <li>Define the difference $Z_N(s) := S_N(s) - I_N(s)$, then</li>
</ul>

<script type="math/tex; mode=display">Z_N(s) = \sum_{n=1}^N\Big(\frac{1}{n^s} - \int_{n-\frac12}^{n+\frac12}\frac1{x^s}\,dx\Big)</script>

<p>is a sum of $N$ items in the first $N$ boxes. The $n$th item $\Delta Z_n(s):=\frac{1}{n^s} - \int_{n-\frac12}^{n+\frac12}\frac1{x^s}\,dx$ is the difference between a term at the $n$th red dot and an integral over the containing blue box.</p>

<p>Next, we realize some nice properties of the sum $Z_N(s)$</p>

<ul>
  <li>
    <p>Looking at each term in the sum $Z_N(s)$</p>

    <script type="math/tex; mode=display">\Delta Z_n(s) := \frac{1}{n^s} - \int_{n-\frac12}^{n+\frac12}\frac1{x^s}\,dx = \int_{-\frac12}^{\frac12}\Big(\frac{1}{n^s}-\frac{1}{(n+x)^s}\Big)dx,</script>

    <p>we find that for any fixed $n$, $\Delta Z_n(s)$ is an analytic function of $s\in\mathbb{C}$.</p>
  </li>
  <li>
    <p>By a comparison with $\sum_{n=1}^\infty\frac{1}{n^{s+2}}$, it is not hard to establish the convergence of $Z_\infty(s) = \sum_{n=1}^\infty\Delta Z_N(s)$ for all $\mathrm{Re}(s)&gt;-1$. This indicates that $Z_\infty(s)$ is an analytic function for all $\mathrm{Re}(s)&gt;-1$.</p>
  </li>
</ul>

<p>With these setup, let’s start our proof, which will be very straightforward.</p>

<ul>
  <li>
    <p>We first notice that</p>

    <script type="math/tex; mode=display">% &lt;![CDATA[
\begin{aligned}I_0(s)&:=\int_0^{\frac12}\frac{1}{x^s}\,dx = \frac{2^{s-1}}{1-s},\qquad \mathrm{Re}(s)<1,\\ J_0(s)&:=\int_{\frac12}^\infty\frac{1}{x^s}\,dx = -\frac{2^{s-1}}{1-s},\qquad \mathrm{Re}(s)>1, \end{aligned} %]]&gt;</script>

    <p>So $I_0(s)$ and $-J_0(s)$ are analytic continuations of each other to the respective half-planes. Consequently, the following two functions are also analytic continuations of each other</p>

    <script type="math/tex; mode=display">% &lt;![CDATA[
\begin{cases}Z_\infty(s)-I_0(s), &-1<\mathrm{Re}(s)<1\\ Z_\infty(s) + J_0(s), &\mathrm{Re}(s)>1\end{cases} %]]&gt;</script>
  </li>
  <li>
    <p>Our goal is to show that</p>

    <script type="math/tex; mode=display">% &lt;![CDATA[
\zeta(s) = Z_\infty(s) - I_0(s),\qquad-1<\mathrm{Re}(s)<1, %]]&gt;</script>

    <p>therefore we will be done once it is shown that $\zeta(s) = Z_\infty(s) + J_0(s)$ for $\mathrm{Re}(s)&gt;1$. This can be shown simply:</p>

    <script type="math/tex; mode=display">% &lt;![CDATA[
\begin{aligned}Z_\infty(s) + J_0(s) &= \lim_{N\to\infty}\Big(\sum_{n=1}^N\frac{1}{n^s} - \int_{\frac12}^{N+\frac{1}{2}}\frac{1}{x^s}\,dx\Big)+\int_{\frac12}^\infty\frac{1}{x^s}\,dx \\ &=\sum_{n=1}^\infty\frac{1}{n^s} + \lim_{N\to\infty}\int_{N+\frac{1}{2}}^\infty\frac{1}{x^s}\,dx\\ &=\zeta(s)+0 \end{aligned} %]]&gt;</script>

    <p>Q.E.D.</p>
  </li>
</ul>

<h3 id="euler-maclaurin-formula-and-the-zeta-function">Euler-Maclaurin Formula and the Zeta Function</h3>

<p>The famous Euler-Maclarin formula is an important tool for approximating integrals on a uniform grid. Probably lesser-known is its connection to the zeta function.</p>

<p>The typical Euler-Maclaurin formula on the integer grid $n=0,1,2,\dots,N$ is</p>

<script type="math/tex; mode=display">\sum_{n=1}^{N-1}f(n)+\frac{f(0)+f(N)}{2} = \int_0^Nf(x)\,dx+\sum_{m=2}^\infty\frac{B_{m}}{m!}\Big(f^{(m-1)}(N)-f^{(m-1)}(0)\Big)</script>

<p>Using the relationship between the the Bernoulli numbers and the zeta function, we can rewrite this as</p>

<script type="math/tex; mode=display">\begin{aligned}
\sum_{m=0}^\infty\frac{\zeta(-m)}{m!}\Big(f^{(m)}(0)+(-1)^mf^{(m)}(N)\Big) = \sum_{n=1}^{N-1}f(n) - \int_0^Nf(x)\,dx
\end{aligned}</script>

<p>It is remarkable that we once again return to the “zeta = sum - integral” relation.</p>

<p>Let’s simplify this further. If we substitute $f(x) = x^k$ into the formula, a lot of the terms become zero and we get</p>

<script type="math/tex; mode=display">\begin{aligned}
\zeta(-k) + \sum_{m=0}^k\frac{\zeta(-m)}{m!}(-1)^mf^{(m)}(N) = \sum_{n=1}^{N-1}n^k - \int_0^Nx^k\,dx
\end{aligned}</script>

<p>Then we let $N\to\infty$ and think of the limiting integral as a <a href="https://en.wikipedia.org/wiki/Hadamard_regularization">Hadamard finite part integral</a> (f.p.), this yields</p>

<script type="math/tex; mode=display">\begin{aligned}
\zeta(-k) = \sum_{n=1}^\infty\frac{1}{n^{-k}} - f.p.\int_0^\infty\frac{1}{x^{-k}}\,dx
\end{aligned}</script>

<p>This is exactly the “zeta = sum - integral” relation! So the Euler-Maclaurin formula is just a special case of this connection after all. (Alternatively, one can think of the finite part integral as two integrals $\int_0^1x^k\,dx$ and $\int_1^\infty x^k\,dx$, then use the analytic continuation idea, like for the $I_0$ and $J_0$ integrals in the above proof, to show that they add to $0$.)</p>

<p>We see that using appropriate notions, the relation</p>

<script type="math/tex; mode=display">\zeta(s) = \sum_{n=1}^\infty\frac{1}{n^s} - \int_0^\infty\frac{1}{x^s}\,dx.</script>

<p>holds for all values of $s\in\mathbb{C}$.</p>

<p>I have written before about <a href="/blog/2021/01/24/regular-and-singular-numerical-integration/">integrating singular functions</a> using this zeta relation, but the Euler-Maclarin formula shows that such relation also works for regular integrals. So, and here is the punchline, an even more beautiful fact is that we can handle both singular and regular integrals categorically using just one zeta relation!<sup id="fnref:sidi"><a href="#fn:sidi" class="footnote">1</a></sup></p>

<div class="footnotes">
  <ol>
    <li id="fn:sidi">
      <p>For more general zeta function connections, see the recent <a href="http://www.cs.technion.ac.il/~asidi/Sidi_Journal_Papers/P130_AQC.Asymp_Expansions.pdf">book chapter</a> by Avraham Sidi. <a href="#fnref:sidi" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Questioning History]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/10/questioning-history/"/>
    <updated>2021-04-10T01:30:19-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/10/questioning-history</id>
    <content type="html"><![CDATA[<p>Lately, I have been listening to this <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast series</a> that has completely changed how I see the history of science.</p>

<p>In the series, Viktor Blåsjö, a professor and an emerging historian of mathematics, told a mind-blowing story about Galileo which is nothing like what we are usually told about. We have been told by the teachers, by the media, and by the mainstream science philosophers, that Galileo was the “father of modern science.” But to the contrary, Galileo was not only poor in mathematics, but also a dilettante physicist. Let me name a few things from the podcast series and you will know how interesting it is:</p>

<!--more-->

<ul>
  <li>Galileo couldn’t even solve some easier math problems, let alone understanding the new developments in math and physics of his contemporary mathematicians. That was why he tried to gain fame by attacking Aristotle from 2000 years ago, like beating a dead horse.</li>
  <li>Galileo picked on Aristotle also because he could not understand Archimedes, the true master of math and physics from Aristotle’s time. Framing Aristotle as the great authority of physics makes it easy for Galileo to defeat. Had he picked on Archimedes, his whole arguments would just fall apart.</li>
  <li>Contrary to the common belief that Galileo took experiements and obervations seriously, he often manipulated the data just to prove his points, made baseless and wrong guesses, and even plagiarized results from other people.</li>
  <li>Why haven’t the mainstream philosophers and historians debunked Galileo already? Because they also lack the skill to appreciate the great work of the mathematicians since Archimedes. Nowadays, <a href="https://youtu.be/LbbQ8SaYy9M?t=702">20 times more</a> papers about Aristotle than about Archimedes are being published under the category of history of science.</li>
</ul>

<p>Of course, all of these sound like wild claims if you are hearing them for the first time. So I would encourage any curious minds to go listen to the <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast</a> or read the corresponding <a href="https://arxiv.org/abs/2102.06595">monograph</a> then judge for themselves.</p>

<p>This new interpretation of Galileo has solved one of my long-standing puzzles for me. In all the serious science textbooks, the most important discoveries are often associated with great mathematicians (such as the principles and laws associated with <a href="https://en.wikipedia.org/wiki/Archimedes%27_principle">Archimedes</a>, <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">Kepler</a>, <a href="https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation">Newton</a>, <a href="https://en.wikipedia.org/wiki/Bernoulli%27s_principle">Bernoulli</a>, <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">Euler</a>, <a href="https://en.wikipedia.org/wiki/Gauss%27s_law">Gauss</a>). Why then has Galileo, “the father of modern science,” never been linked to any of those physical laws, other than the folklore of the “Pisa experiment”? Because he lacked the mathematical ability to do serious physics.</p>

<p>Studying the history of mathematics allows us to understand the more true history of science. But there are more reasons to study the history.</p>

<h3 id="why-study-history-of-mathematics">Why study history of mathematics?</h3>

<p>We know that Leibniz was one of the inventors of calculus. But do you know that Leibniz did not actually prove the fundamental theorem of calculus? In fact, he did not worry much about the foundation of calculus, certain not the “rigorous definition” of derivatives.</p>

<p>What Leibniz cared most about was the <em>transcendental curves</em>, that is, the graphs of transcendental functions. According to the inspiring book <a href="https://www.sciencedirect.com/book/9780128132371"><em>Transcendental Curves in the Leibnizian Calculus</em></a>, again by Blåsjö, “the problem of transcendental curves was to him (Leibniz) the guiding star for the better part of his mathematical works throughout his life.”</p>

<p>To Leibniz, functions like $\log(x)$ and $\cosh(x)$ are nothing but notations; the claim that $y=\cosh(x)$ solves the equation $y’'=y$ makes no sense. The important thing to Leibniz was that someone can actually graph a function like $\log(x)$, or calculate its value given any input $x$. After all, what’s the point of writing the symbol “$\log(x)$,” or naming a function “hyperbolic,” if you can’t even find their values? Therefore, to Leibniz, an expression like $\int_1^x \frac{1}{t}\,dt$ makes much more sense than $\log(x)$; being able to draw a <a href="https://en.wikipedia.org/wiki/Catenary">catenary</a> by hanging a chain is more important than simply writing $\cosh(x)=(e^x+e^{-x})/2$.</p>

<p>The same can be said about the other common transcendental functions such as $\sin(x)$ and $\cos(x)$. Indeed, students are being told that sinusoidal functions are “elementary functions,” but how “elementary” is it if you can’t even tell what $\sin(1)$ is right away? People still love to see animations, such as <a href="https://twitter.com/LucasVB/status/1378529237322334208?s=20">this one</a> and <a href="https://twitter.com/mathladyhazel/status/1348876985053958147?s=20">this one</a>, that can help them understand what trigonometric functions are.</p>

<p>The discussions above give us another important reason to study the history of mathematics. History tells us what are the “natural” things to pay attention to when we first teach or learn a mathematical concept. Knowing why we need transendental functions in the first place is <em>understanding</em>; telling people that these functions are elementary is <em>indoctrination</em>.</p>

<p>True education is about understanding. Unfortunately, a lot of the schooling nowadays are about indoctrination (and probably day-care as well).</p>

<h3 id="right-way-to-study-history">Right way to study history</h3>

<p>In Blåsjö’s <a href="http://intellectualmathematics.com/history-of-mathematics/">History of Mathematics</a> course, he contrasted <a href="https://youtu.be/SX0SpjAJJ9M">two ways of studying history of thought</a>:</p>

<ul>
  <li><strong>The bad way</strong>: to enforce our own beliefs and ways-of-thinking on the people in the past. For example, a bad historian might ask:
    <ul>
      <li>Who first discover this thing that I believe in?</li>
      <li>When did people start thinking like us?</li>
    </ul>
  </li>
  <li><strong>The good way</strong>: to embrace the diversity of thoughts, be curious about different perspectives. For example, a good historian might ask:
    <ul>
      <li>Why do people used to think differently?</li>
      <li>Why did their way of thinking make more sense at their time?</li>
    </ul>
  </li>
</ul>

<p>Historians who enforce their own beliefs on people from the past cannot find out the true history of Galileo, let alone the true history of science. Teachers who enforce their own thoughts on students cannot inspire anyone to understand.</p>

<p>Next time you are told that somethings are conventional or have been the way they are, just remember <a href="https://twitter.com/AmuseChimp/status/906147488582787073?s=20">this powerful quote from an amused chimp</a>:</p>

<blockquote>
  <p>If the news are fake, imagine history.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Is Operational Calculus? Finite Differences and the Euler-Maclaurin Formula]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus/"/>
    <updated>2021-04-04T23:45:42-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus</id>
    <content type="html"><![CDATA[<p>When we first learned ordinary differential equations (ODEs) in class, we were told that the solution of</p>

<script type="math/tex; mode=display">f'(x)=f(x)</script>

<p>is $f(x)=Ce^x$ for an arbitrary constant $C$. We were told that this solution can be obtained either by observation and guessing (then it’s easy to verify solution), or by a separation of variable (e.g. integrate $dy/y=dx$). But these approaches don’t tell us what to do for higher-order equations, such as $f’'-3f’+2f=0$.</p>

<p>The <strong>operational calculus</strong> provides a convenient way to algebraically derive the solution to these linear ODEs. The derivation goes as follows.</p>

<!--more-->

<ul>
  <li>We want to solve the ODE $f’(x)=f(x)$, or equivalently $(\frac{d}{dx}-1)f(x)=0$.</li>
  <li>Define the differential operator $D:=\frac{d}{dx}$, then the equation becomes $(D-1)f(x)=0$.</li>
  <li>Define the integral operator $J$ such that $Jf(x):=\int_0^xf(t)dt$. (Therefore by the fundamental theorem of calculus, for any smooth function $g(x)$ we have $DJg(x)= g(x)$ and $JDg(x)=g(x)+C$, where the constant $C=-g(0)$. In particular, if $g(0)=0$ then we have $DJg(x)=JDg(x)=g(x)$, so we can write $J=D^{-1}$.)</li>
  <li>Rewrite the ODE as $\frac{1-D^{-1}}{D^{-1}}f(x)=0$, so $f(x)=\frac{D^{-1}}{1-D^{-1}}0 = \frac{1}{1-J}(D^{-1}0)$</li>
  <li>Suppose the above $0$ function is the derivative of some constant $C$, that is, $D^{-1}0=C$, then $f(x)=\frac{1}{1-J}C=C\left(\frac{1}{1-J}1\right)$</li>
  <li>By the Taylor series we have $\frac{1}{1-J}=1 + J+J^2+\dots=\sum_{n=0}^\infty J^{n}$, we can write $f(x)=C\sum_{n=0}^\infty J^{n}1$</li>
  <li>By induction, $J1=\int_0^x1\,dt=x,\; J^21=\int_0^xt\,dt=\frac{x^2}{2},\dots,J^n1=\frac{x^n}{n!}$. Therefore $f(x)=C\sum_{n=0}^\infty \frac{x^n}{n!}=C e^x$.</li>
</ul>

<p>Similarly we can show that $(D-a)f(x)=0$ has a solution $f(x)=Ce^{ax}$. Then for a higher-order equation such as $f’'-3f’+2f=0$, we can rewrite it as $(D^2-3D+2)f=0$ which can be then factored into $(D-1)(D-2)f(x)=0$, implying either $(D-1)f=0$ or $(D-2)f=0$.</p>

<p>Operational calculus reduces a differential equation into an algebraic equation, which is often much easier to solve.</p>

<h3 id="the-calculus-of-finite-differences">The calculus of finite differences</h3>

<p>The calculus of finite differences is a brilliant extension of the operational calculus to discrete quantities. The most important observation is that, given the <em>shift operator</em></p>

<script type="math/tex; mode=display">Ef(x):=f(x+h),</script>

<p>there is this remarkable <em>exponential map</em>, $E=\exp(hD)$, that connects the continuous and discrete worlds. This exponential map is simply a different way of writing the Taylor series expansion:</p>

<script type="math/tex; mode=display">Ef(x)=\exp(hD)f(x)=\left(1+hD+\frac{(hD)^2}{2!}+\frac{(hD)^3}{3!}+\dots\right)f(x)</script>

<p>From here, a lot of the great things can happen. For example, if we want to derive a <em>forward difference</em> formula such as</p>

<script type="math/tex; mode=display">f'(x)\approx \frac{f(x+h)-f(x)}{h},</script>

<p>we can define the forward difference operator</p>

<script type="math/tex; mode=display">\Delta f(x):=f(x+h)-f(x)=(E-1)f(x)</script>

<p>and write down the relationship</p>

<script type="math/tex; mode=display">\Delta+1=E=\exp(hD).</script>

<p>Then it follows naturally that</p>

<script type="math/tex; mode=display">D=\frac{1}{h}\log(1+\Delta)=\frac{1}{h}\left(\Delta - \frac{\Delta^2}{2} + \frac{\Delta^3}{3} - \dots\right)</script>

<p>Truncating this series at the first term gives back the first-order difference formula $f’(x)\approx\frac{1}{h}\Delta f(x)$ as before; truncating at the second term gives the second-order formula</p>

<script type="math/tex; mode=display">f'(x)\approx\frac{1}{h}(\Delta-\Delta^2/2)f(x)=\frac{1}{h}\left(-\frac{1}{2}f(x+2h)+2f(x+h)−\frac{3}{2}f(x)\right)</script>

<p>and so on.</p>

<h3 id="the-euler-maclaurin-formula">The Euler-Maclaurin formula</h3>

<p>Another striking application of such finite difference calculus is to derive the more advanced <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula"><em>Euler-Maclaurin formula</em></a>, which is in some sense a discrete version of the fundamental theorem of calculus.</p>

<p>As we have mentioned in the first part of this article, the fundamental theorem of calculus can be written in terms of operators as</p>

<script type="math/tex; mode=display">D^{-1}f(x)=Jf(x)+C=\int_0^xf(t)\,dt+C</script>

<p>So we would like to know if we replace the differential operator $D$ with the difference operator $\Delta$, how would this relationship change? What is $\Delta^{-1}f(x)$?</p>

<p>We have shown previously that $1+\Delta=\exp(hD)$, so it would be naturaly to write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{1}{\exp(hD)-1}.</script>

<p>However, the above right-hand side corresponds to the function $1/(e^x-1)$ which is singular at the origin, so instead we write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{hD}{\exp(hD)-1}(hD)^{-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}(hD)^{n}(hD)^{-1}.</script>

<p>Here we have used the fact that</p>

<script type="math/tex; mode=display">\frac{x}{e^x-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}x^n</script>

<p>where $B_n$ are the <a href="https://en.wikipedia.org/wiki/Bernoulli_numbers">Bernoulli numbers</a>. Notice that with our notations, we have</p>

<script type="math/tex; mode=display">(hD)^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C</script>

<p>and</p>

<script type="math/tex; mode=display">(hD)^{n}(hD)^{-1}f(x)=h^{n-1}f^{(n-1)}(x)</script>

<p>therefore</p>

<script type="math/tex; mode=display">\Delta^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}f^{(n-1)}(x)</script>

<p>Now apply $\Delta$ on both sides, we have</p>

<script type="math/tex; mode=display">f(x)=\frac{1}{h}\int_x^{x+h}f(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}(f^{(n-1)}(x+h)-f^{(n-1)}(x))</script>

<p>This is one version of the Euler-Maclaurin formula. Finally, if we replace $x$ by $a,\;a+h,\;a+2h,\;\dots,\;a+(m-1)h$, one-by-one, and sum all the resulting formulae together, we get</p>

<script type="math/tex; mode=display">\sum_{k=0}^{m-1}f(a+kh)=\frac{1}{h}\int_a^bf(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}\left(f^{(n-1)}(b)-f^{(n-1)}(a)\right)</script>

<p>where $b:=a+mh$. So here we are, this gives us the usual form of the Euler-Maclaurin formula.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[It Is Okay to Speed Up a Little]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little/"/>
    <updated>2021-03-26T00:57:22-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little</id>
    <content type="html"><![CDATA[<p>We naturally avoid making mistakes. If you were a caveman who survives by hunting, you’d try not to make mistakes that could get you killed by animals.</p>

<p>But in the modern days, we no longer worry about ferocious animals. We now have much safer spaces that are tolerant of our mistakes. So we can afford to make more mistakes. In fact, making mistakes is how we learn.</p>

<p>Yet, people avoid mistakes because it makes you “look bad.” But there is no learning without making mistakes. So how can we gain the courage to learn without worrying about the mistakes we would make?</p>

<p>I think math has the answer.</p>

<!--more-->

<p>In numerical analysis, we design algorithms that solve problems quickly and accurately. One major task when developing such an algorithm is to combat with numerical errors. If you open any classical numerical analysis textbook, you will likely see discussions of error analysis: how to eliminate or reduce roundoff errors, and how to prevent errors from propagating and contaminating the final results.</p>

<p>Talking about errors can sound bad and boring. It may drive beginners away from numerical analysis before they can learn the true beauty of the subject.</p>

<p>In fact, mathematicians know about the bad connotations associated with “errors.” When communicating with fellow mathematicians, we don’t talk about errors as often as one would imagine. For example, instead of saying “the algorithm makes smaller errors” we like to say “the algorithm converges faster.” And instead of “the algorithm prevents error propagation” we like to say “the algorithm is stable and robust.”</p>

<p>“Fast convergence” and “smaller errors” mean the same thing, but the framings are completely different. “Convergence” focuses on the positive effects and makes people excited. If you ask any numerical analyst what makes them love their field, I’ll bet you no one would say they love the error analysis; rather, they would tell you about the lightning computation speeds, the simple ideas behind powerful algorithms, the unexpected connections between ideas from different fields, and more.</p>

<p>Likewise, “making mistakes” gets its own bad rap. I propose replacing it with “speeding up” because making more mistakes allows you to learn quicker. Whenever you are stuck and can’t make any progress because you are afraid of making mistakes, try to tell yourself “I am just going to speed up my learning a little.”</p>

<p>Making mistakes can be embarrassing, but <a href="https://twitter.com/edlatimore/status/1058305553020141570">embarrassment is simply the cost of entry</a>. The embarrassment you feel only exist in your mind, learning is what can actually happen. Once you allow yourself to look stupid and take on the beginner’s mindset, learning will be unstoppable.</p>

<p>Make more mistakes. It is okay to speed up a little, and it feels great.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Low-rank and Rank Structured Matrices]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices/"/>
    <updated>2021-03-21T15:33:24-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices</id>
    <content type="html"><![CDATA[<p>When you begin to research a field of study, you often get overwhelmed by the amount of existing knowledge you have to learn before you could go further. One useful way to bootstrap yourself is to only learn a minimal amount of basic ideas that are enough for you to “survive”  in the field. Such basic ideas are the Minimal Actionable Knowledge &amp; Experience (MAKE) of the field. Here, I will try to present the “MAKE” of the field of fast matrix computations.</p>

<!--more-->

<h3 id="low-rank-matrices-and-important-information">Low-rank matrices and important information</h3>

<p>An $m\times n$  matrix $\mathbf{A}$ is low-rank if its rank, $k\equiv\mathrm{rank}\,\mathbf{A}$, is far less than $m$ and $n$. Then $\mathbf{A}$ has a factorization <script type="math/tex">\mathbf{A} =\mathbf{E}\mathbf{F}</script> where $\mathbf{E}$ is a tall-skinny matrix with $k$ columns and $\mathbf{F}$ a short-fat matrix with $k$ rows.</p>

<p><img class="center" src="/images/blog_figures/lowRankFac.png" width="500"></p>

<p>For example the following $3\times3$ matrix is of rank-$1$ only.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{bmatrix}
1 & 2 & 3\\
1 & 2 & 3\\
1 & 2 & 3
\end{bmatrix}
=
\begin{bmatrix}
1\\
1\\
1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix} %]]&gt;</script>

<p>Given a matrix $\mathbf{A}$, there are many ways to find $\mathrm{rank}\,\mathbf{A}$. One way is to find the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a></p>

<script type="math/tex; mode=display">\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*</script>

<p>where $\mathbf{\Sigma}=\mathrm{diag}(\sigma_1,\sigma_2,\dots)$ is an $m\times n$ diagonal matrix, whose diagonal elements are called the singular values of $\mathbf{A}$. Then $\mathrm{rank}\,\mathbf{A}$ is the number of nonzero singular values.</p>

<p>The SVD tells you the most important information about a matrix: the <a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_spectral_norm)">Eckart-Young theorem</a> says that the best rank-$k$ approximation of $\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*$ can be obtained by only keeping the first $k$ singular values and zeroing out the rest in $\mathbf{\Sigma}$. When the singular values decay quickly, such a low-rank approximation can be very accurate. This is particularly important in practice when we want to solve problems efficiently by ignoring the unimportant information.</p>

<p>An interesting example is the $n\times n$ <a href="https://en.wikipedia.org/wiki/Hilbert_matrix">Hilbert matrix</a> $\mathbf{H}_n$, whose $(i,j)$ entry is defined to be $\frac{1}{i+j-1}$. $\mathbf{H}_n$ is full-rank for any size $n$, but it is <em>numerically low-rank</em>, meaning that its singular values decay rapidly such that given any small threshold $\epsilon$, only a few singular values are above $\epsilon$. For example with $\epsilon=10^{-15}$, the $1000\times1000$ has numerical rank $28$.</p>

<p>Other examples of (numerically) low-rank matrices include the <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde</a>, <a href="https://en.wikipedia.org/wiki/Cauchy_matrix">Cauchy</a>, <a href="https://en.wikipedia.org/wiki/Hankel_matrix">Hankel</a>, and <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz</a> matrices, as well as matrices constructed from smooth data or smooth functions.</p>

<p>As it turns out, a lot of the matrices we encounter in practice are numerically low-rank. So finding low-rank approximation (e.g. in the form of $\mathbf{A}=\mathbf{EF}$ at the beginning) is one of the most important and fundamental subjects in applied math nowadays.</p>

<h3 id="data-sparsity-and-rank-structured-matrices">Data sparsity and rank structured matrices</h3>

<p>Matrix sizes have been growing with technological advancements. Many common matrix algorithms scale cubically with the matrix size, meaning that even if your computing power grows 1000 times, you could only afford to solve problems that are 10 times bigger than before. These common algorithms include matrix multiplication, matrix inversion, and matrix factorizations (e.g. LU, QR, SVD). Therefore, it is important to speed up these matrix computation methods in order to fully exploit the ever growing computing power.</p>

<p>One major strategy to accelerate the computations is to exploit the <strong>data sparsity</strong> of a matrix. Data sparsity is a deliberately vague concept which broadly refers to the kind of internal structures in a matrix that can help make computations faster. Following are some common examples of data-sparse matrices.</p>

<ul>
  <li>The most classcial data-sparse matrices are the <strong>sparse matrices</strong>, ones with a large number of zero entries. Using <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">compressed data formats</a>, you can save a lot of memories by storing only the nonzero entries (together with their positions). You can greatly reduce computation time by only operating on the nonzero entries while maintaining the sparsity of the matrices (i.e., avoid introducing more nonzero entries). Common sparse matrices include diagonal/block-diagonal matrices, banded matrices, permutations, adjacency matrix of graphs, etc.</li>
  <li><strong>Low-rank matrices</strong>, as we have introduced above, are ones admit low-rank factorizations. Commonly used factorizations include the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs">reduced SVDs</a>, <a href="https://en.wikipedia.org/wiki/Interpolative_decomposition">interpolative decomposition</a>, <a href="https://en.wikipedia.org/wiki/CUR_matrix_approximation">CUR decomposition</a>, <a href="https://en.wikipedia.org/wiki/RRQR_factorization">rank-revealing QR factorizations</a>, etc. Normally the SVD algorithms are more expensive, so the other algorithms are preferred when possible; for very large matrices, all the factorization algorithms can be further accelerated by <a href="https://epubs.siam.org/doi/abs/10.1137/090771806">randomization techniques</a>.</li>
  <li><strong>Rank structured matrices.</strong> These matrices are not necessarily low-rank, but can be split into a relatively small number of submatrices, each of which is low-rank. For example, the picture below shows the structure of a <em>Hierarchically Off-diagonal Low Rank</em> (HODLR) matrix, where all the off-diagonal blocks, big or small, have similar ranks. Such structure can for example arise from gravitational or electrostatic interactions, where the diagonal blocks represent the local interactions and the off-diagonal blocks represent the far interactions; the far interactions are low-rank because they are much smoother than the local interactions. Other rank structured matrices include the <em>Hierarchically Semi-separable</em> (HSS) matrices, the inverse of banded matrices, and the more general <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix">$\mathcal{H}$-matrices</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix#H2-matrices">$\mathcal{H}^2$-matrices</a>. Rank structured matrices can be efficiently handled using tree structures. Matrix algorithms designed for these matrices can be very fast, with computation time scaling linearly or log-linearly with the matrix size.</li>
</ul>

<p><img class="center" src="/images/blog_figures/HODLR.png" width="400"></p>

<ul>
  <li><strong>Complementary low-rank matrices</strong> are a special type of rank structured matrices that can be decomposed by the <em>butterfly factorization</em> (BF). The BF is inspired by ideas of the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> algorithm (divide-and-conquer and permutations), which can be explained using the <a href="https://en.wikipedia.org/wiki/Butterfly_diagram">butterfly diagram</a>. Butterfly algorithms were initially motivated by solving oscillatory problems such as wave scattering.</li>
</ul>

<p>With these ideas above, plus a little coding experience with some simple rank structured matrices (a good place to start is with the first two of these <a href="https://amath.colorado.edu/faculty/martinss/2014_CBMS/codes.html">tutorial codes</a>), you are equipped with the “MAKE” that gets you ready for going on an advanture to the fast computations with matrices. All the details and other more advanced topics can be learned later once you dig far enough.</p>
]]></content>
  </entry>
  
</feed>
