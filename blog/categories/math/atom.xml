<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Bobbie's Blog]]></title>
  <link href="http://bobbielf2.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://bobbielf2.github.io/"/>
  <updated>2021-04-14T19:58:56-05:00</updated>
  <id>http://bobbielf2.github.io/</id>
  <author>
    <name><![CDATA[Bowei "Bobbie" Wu .]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Questioning History]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/10/questioning-history/"/>
    <updated>2021-04-10T01:30:19-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/10/questioning-history</id>
    <content type="html"><![CDATA[<p>Lately, I have been listening to this <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast series</a> that has completely changed how I see the history of science.</p>

<p>In the series, Viktor Blåsjö, a professor and an emerging historian of mathematics, told a mind-blowing story about Galileo which is nothing like what we are usually told about. We have been told by the teachers, by the media, and by the mainstream science philosophers, that Galileo was the “father of modern science.” But to the contrary, Galileo was not only poor in mathematics, but also a dilettante physicist. Let me name a few things from the podcast series and you will know how interesting it is:</p>

<!--more-->

<ul>
  <li>Galileo couldn’t even solve some easier math problems, let alone understanding the new developments in math and physics of his contemporary mathematicians. That was why he tried to gain fame by attacking Aristotle from 2000 years ago, like beating a dead horse.</li>
  <li>Galileo picked on Aristotle also because he could not understand Archimedes, the true master of math and physics from Aristotle’s time. Framing Aristotle as the great authority of physics makes it easy for Galileo to defeat. Had he picked on Archimedes, his whole arguments would just fall apart.</li>
  <li>Contrary to the common belief that Galileo took experiements and obervations seriously, he often manipulated the data just to prove his points, made baseless and wrong guesses, and even plagiarized results from other people.</li>
  <li>Why haven’t the mainstream philosophers and historians debunked Galileo already? Because they also lack the skill to appreciate the great work of the mathematicians since Archimedes. Nowadays, <a href="https://youtu.be/LbbQ8SaYy9M?t=702">20 times more</a> papers about Aristotle than about Archimedes are being published under the category of history of science.</li>
</ul>

<p>Of course, all of these sound like wild claims if you are hearing them for the first time. So I would encourage any curious minds to go listen to the <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast</a> or read the corresponding <a href="https://arxiv.org/abs/2102.06595">monograph</a> then judge for themselves.</p>

<p>This new interpretation of Galileo has solved one of my long-standing puzzles for me. In all the serious science textbooks, the most important discoveries are often associated with great mathematicians (such as the principles and laws associated with <a href="https://en.wikipedia.org/wiki/Archimedes%27_principle">Archimedes</a>, <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">Kepler</a>, <a href="https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation">Newton</a>, <a href="https://en.wikipedia.org/wiki/Bernoulli%27s_principle">Bernoulli</a>, <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">Euler</a>, <a href="https://en.wikipedia.org/wiki/Gauss%27s_law">Gauss</a>). Why then has Galileo, “the father of modern science,” never been linked to any of those physical laws, other than the folklore of the “Pisa experiment”? Because he lacked the mathematical ability to do serious physics.</p>

<p>Studying the history of mathematics allows us to understand the more true history of science. But there are more reasons to study the history.</p>

<h3 id="why-study-history-of-mathematics">Why study history of mathematics?</h3>

<p>We know that Leibniz was one of the inventors of calculus. But do you know that Leibniz did not actually prove the fundamental theorem of calculus? In fact, he did not worry much about the foundation of calculus, certain not the “rigorous definition” of derivatives.</p>

<p>What Leibniz cared most about was the <em>transcendental curves</em>, that is, the graphs of transcendental functions. According to the inspiring book <a href="https://www.sciencedirect.com/book/9780128132371"><em>Transcendental Curves in the Leibnizian Calculus</em></a>, again by Blåsjö, “the problem of transcendental curves was to him (Leibniz) the guiding star for the better part of his mathematical works throughout his life.”</p>

<p>To Leibniz, functions like $\log(x)$ and $\cosh(x)$ are nothing but notations; the claim that $y=\cosh(x)$ solves the equation $y’'=y$ makes no sense. The important thing to Leibniz was that someone can actually graph a function like $\log(x)$, or calculate its value given any input $x$. After all, what’s the point of writing the symbol “$\log(x)$,” or naming a function “hyperbolic,” if you can’t even find their values? Therefore, to Leibniz, an expression like $\int_1^x \frac{1}{t}\,dt$ makes much more sense than $\log(x)$; being able to draw a <a href="https://en.wikipedia.org/wiki/Catenary">catenary</a> by hanging a chain is more important than simply writing $\cosh(x)=(e^x+e^{-x})/2$.</p>

<p>The same can be said about the other common transcendental functions such as $\sin(x)$ and $\cos(x)$. Indeed, students are being told that sinusoidal functions are “elementary functions,” but how “elementary” is it if you can’t even tell what $\sin(1)$ is right away? People still love to see animations, such as <a href="https://twitter.com/LucasVB/status/1378529237322334208?s=20">this one</a> and <a href="https://twitter.com/mathladyhazel/status/1348876985053958147?s=20">this one</a>, that can help them understand what trigonometric functions are.</p>

<p>The discussions above give us another important reason to study the history of mathematics. History tells us what are the “natural” things to pay attention to when we first teach or learn a mathematical concept. Knowing why we need transendental functions in the first place is <em>understanding</em>; telling people that these functions are elementary is <em>indoctrination</em>.</p>

<p>True education is about understanding. Unfortunately, a lot of the schooling nowadays are about indoctrination (and probably day-care as well).</p>

<h3 id="right-way-to-study-history">Right way to study history</h3>

<p>In Blåsjö’s <a href="http://intellectualmathematics.com/history-of-mathematics/">History of Mathematics</a> course, he contrasted <a href="https://youtu.be/SX0SpjAJJ9M">two ways of studying history of thought</a>:</p>

<ul>
  <li><strong>The bad way</strong>: to enforce our own beliefs and ways-of-thinking on the people in the past. For example, a bad historian might ask:
    <ul>
      <li>Who first discover this thing that I believe in?</li>
      <li>When did people start thinking like us?</li>
    </ul>
  </li>
  <li><strong>The good way</strong>: to embrace the diversity of thoughts, be curious about different perspectives. For example, a good historian might ask:
    <ul>
      <li>Why do people used to think differently?</li>
      <li>Why did their way of thinking make more sense at their time?</li>
    </ul>
  </li>
</ul>

<p>Historians who enforce their own beliefs on people from the past cannot find out the true history of Galileo, let alone the true history of science. Teachers who enforce their own thoughts on students cannot inspire anyone to understand.</p>

<p>Next time you are told that somethings are conventional or have been the way they are, just remember <a href="https://twitter.com/AmuseChimp/status/906147488582787073?s=20">this powerful quote from an amused chimp</a>:</p>

<blockquote>
  <p>If the news are fake, imagine history.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Is Operational Calculus? Finite Differences and the Euler-Maclaurin Formula]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus/"/>
    <updated>2021-04-04T23:45:42-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus</id>
    <content type="html"><![CDATA[<p>When we first learned ordinary differential equations (ODEs) in class, we were told that the solution of</p>

<script type="math/tex; mode=display">f'(x)=f(x)</script>

<p>is $f(x)=Ce^x$ for an arbitrary constant $C$. We were told that this solution can be obtained either by observation and guessing (then it’s easy to verify solution), or by a separation of variable (e.g. integrate $dy/y=dx$). But these approaches don’t tell us what to do for higher-order equations, such as $f’'-3f’+2f=0$.</p>

<p>The <strong>operational calculus</strong> provides a convenient way to algebraically derive the solution to these linear ODEs. The derivation goes as follows.</p>

<!--more-->

<ul>
  <li>We want to solve the ODE $f’(x)=f(x)$, or equivalently $(\frac{d}{dx}-1)f(x)=0$.</li>
  <li>Define the differential operator $D:=\frac{d}{dx}$, then the equation becomes $(D-1)f(x)=0$.</li>
  <li>Define the integral operator $J$ such that $Jf(x):=\int_0^xf(t)dt$. (Therefore by the fundamental theorem of calculus, for any smooth function $g(x)$ we have $DJg(x)= g(x)$ and $JDg(x)=g(x)+C$, where the constant $C=-g(0)$. In particular, if $g(0)=0$ then we have $DJg(x)=JDg(x)=g(x)$, so we can write $J=D^{-1}$.)</li>
  <li>Rewrite the ODE as $\frac{1-D^{-1}}{D^{-1}}f(x)=0$, so $f(x)=\frac{D^{-1}}{1-D^{-1}}0 = \frac{1}{1-J}(D^{-1}0)$</li>
  <li>Suppose the above $0$ function is the derivative of some constant $C$, that is, $D^{-1}0=C$, then $f(x)=\frac{1}{1-J}C=C\left(\frac{1}{1-J}1\right)$</li>
  <li>By the Taylor series we have $\frac{1}{1-J}=1 + J+J^2+\dots=\sum_{n=0}^\infty J^{n}$, we can write $f(x)=C\sum_{n=0}^\infty J^{n}1$</li>
  <li>By induction, $J1=\int_0^x1\,dt=x,\; J^21=\int_0^xt\,dt=\frac{x^2}{2},\dots,J^n1=\frac{x^n}{n!}$. Therefore $f(x)=C\sum_{n=0}^\infty \frac{x^n}{n!}=C e^x$.</li>
</ul>

<p>Similarly we can show that $(D-a)f(x)=0$ has a solution $f(x)=Ce^{ax}$. Then for a higher-order equation such as $f’'-3f’+2f=0$, we can rewrite it as $(D^2-3D+2)f=0$ which can be then factored into $(D-1)(D-2)f(x)=0$, implying either $(D-1)f=0$ or $(D-2)f=0$.</p>

<p>Operational calculus reduces a differential equation into an algebraic equation, which is often much easier to solve.</p>

<h3 id="the-calculus-of-finite-differences">The calculus of finite differences</h3>

<p>The calculus of finite differences is a brilliant extension of the operational calculus to discrete quantities. The most important observation is that, given the <em>shift operator</em></p>

<script type="math/tex; mode=display">Ef(x):=f(x+h),</script>

<p>there is this remarkable <em>exponential map</em>, $E=\exp(hD)$, that connects the continuous and discrete worlds. This exponential map is simply a different way of writing the Taylor series expansion:</p>

<script type="math/tex; mode=display">Ef(x)=\exp(hD)f(x)=\left(1+hD+\frac{(hD)^2}{2!}+\frac{(hD)^3}{3!}+\dots\right)f(x)</script>

<p>From here, a lot of the great things can happen. For example, if we want to derive a <em>forward difference</em> formula such as</p>

<script type="math/tex; mode=display">f'(x)\approx \frac{f(x+h)-f(x)}{h},</script>

<p>we can define the forward difference operator</p>

<script type="math/tex; mode=display">\Delta f(x):=f(x+h)-f(x)=(E-1)f(x)</script>

<p>and write down the relationship</p>

<script type="math/tex; mode=display">\Delta+1=E=\exp(hD).</script>

<p>Then it follows naturally that</p>

<script type="math/tex; mode=display">D=\frac{1}{h}\log(1+\Delta)=\frac{1}{h}\left(\Delta - \frac{\Delta^2}{2} + \frac{\Delta^3}{3} - \dots\right)</script>

<p>Truncating this series at the first term gives back the first-order difference formula $f’(x)\approx\frac{1}{h}\Delta f(x)$ as before; truncating at the second term gives the second-order formula</p>

<script type="math/tex; mode=display">f'(x)\approx\frac{1}{h}(\Delta-\Delta^2/2)f(x)=\frac{1}{h}\left(-\frac{1}{2}f(x+2h)+2f(x+h)−\frac{3}{2}f(x)\right)</script>

<p>and so on.</p>

<h3 id="the-euler-maclaurin-formula">The Euler-Maclaurin formula</h3>

<p>Another striking application of such finite difference calculus is to derive the more advanced <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula"><em>Euler-Maclaurin formula</em></a>, which is in some sense a discrete version of the fundamental theorem of calculus.</p>

<p>As we have mentioned in the first part of this article, the fundamental theorem of calculus can be written in terms of operators as</p>

<script type="math/tex; mode=display">D^{-1}f(x)=Jf(x)+C=\int_0^xf(t)\,dt+C</script>

<p>So we would like to know if we replace the differential operator $D$ with the difference operator $\Delta$, how would this relationship change? What is $\Delta^{-1}f(x)$?</p>

<p>We have shown previously that $1+\Delta=\exp(hD)$, so it would be naturaly to write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{1}{\exp(hD)-1}.</script>

<p>However, the above right-hand side corresponds to the function $1/(e^x-1)$ which is singular at the origin, so instead we write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{hD}{\exp(hD)-1}(hD)^{-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}(hD)^{n}(hD)^{-1}.</script>

<p>Here we have used the fact that</p>

<script type="math/tex; mode=display">\frac{x}{e^x-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}x^n</script>

<p>where $B_n$ are the <a href="https://en.wikipedia.org/wiki/Bernoulli_numbers">Bernoulli numbers</a>. Notice that with our notations, we have</p>

<script type="math/tex; mode=display">(hD)^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C</script>

<p>and</p>

<script type="math/tex; mode=display">(hD)^{n}(hD)^{-1}f(x)=h^{n-1}f^{(n-1)}(x)</script>

<p>therefore</p>

<script type="math/tex; mode=display">\Delta^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}f^{(n-1)}(x)</script>

<p>Now apply $\Delta$ on both sides, we have</p>

<script type="math/tex; mode=display">f(x)=\frac{1}{h}\int_x^{x+h}f(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}(f^{(n-1)}(x+h)-f^{(n-1)}(x))</script>

<p>This is one version of the Euler-Maclaurin formula. Finally, if we replace $x$ by $a,\;a+h,\;a+2h,\;\dots,\;a+(m-1)h$, one-by-one, and sum all the resulting formulae together, we get</p>

<script type="math/tex; mode=display">\sum_{k=0}^{m-1}f(a+kh)=\frac{1}{h}\int_a^bf(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}\left(f^{(n-1)}(b)-f^{(n-1)}(a)\right)</script>

<p>where $b:=a+mh$. So here we are, this gives us the usual form of the Euler-Maclaurin formula.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[It Is Okay to Speed Up a Little]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little/"/>
    <updated>2021-03-26T00:57:22-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little</id>
    <content type="html"><![CDATA[<p>We naturally avoid making mistakes. If you were a caveman who survives by hunting, you’d try not to make mistakes that could get you killed by animals.</p>

<p>But in the modern days, we no longer worry about ferocious animals. We now have much safer spaces that are tolerant of our mistakes. So we can afford to make more mistakes. In fact, making mistakes is how we learn.</p>

<p>Yet, people avoid mistakes because it makes you “look bad.” But there is no learning without making mistakes. So how can we gain the courage to learn without worrying about the mistakes we would make?</p>

<p>I think math has the answer.</p>

<!--more-->

<p>In numerical analysis, we design algorithms that solve problems quickly and accurately. One major task when developing such an algorithm is to combat with numerical errors. If you open any classical numerical analysis textbook, you will likely see discussions of error analysis: how to eliminate or reduce roundoff errors, and how to prevent errors from propagating and contaminating the final results.</p>

<p>Talking about errors can sound bad and boring. It may drive beginners away from numerical analysis before they can learn the true beauty of the subject.</p>

<p>In fact, mathematicians know about the bad connotations associated with “errors.” When communicating with fellow mathematicians, we don’t talk about errors as often as one would imagine. For example, instead of saying “the algorithm makes smaller errors” we like to say “the algorithm converges faster.” And instead of “the algorithm prevents error propagation” we like to say “the algorithm is stable and robust.”</p>

<p>“Fast convergence” and “smaller errors” mean the same thing, but the framings are completely different. “Convergence” focuses on the positive effects and makes people excited. If you ask any numerical analyst what makes them love their field, I’ll bet you no one would say they love the error analysis; rather, they would tell you about the lightning computation speeds, the simple ideas behind powerful algorithms, the unexpected connections between ideas from different fields, and more.</p>

<p>Likewise, “making mistakes” gets its own bad rap. I propose replacing it with “speeding up” because making more mistakes allows you to learn quicker. Whenever you are stuck and can’t make any progress because you are afraid of making mistakes, try to tell yourself “I am just going to speed up my learning a little.”</p>

<p>Making mistakes can be embarrassing, but <a href="https://twitter.com/edlatimore/status/1058305553020141570">embarrassment is simply the cost of entry</a>. The embarrassment you feel only exist in your mind, learning is what can actually happen. Once you allow yourself to look stupid and take on the beginner’s mindset, learning will be unstoppable.</p>

<p>Make more mistakes. It is okay to speed up a little, and it feels great.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Low-rank and Rank Structured Matrices]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices/"/>
    <updated>2021-03-21T15:33:24-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices</id>
    <content type="html"><![CDATA[<p>When you begin to research a field of study, you often get overwhelmed by the amount of existing knowledge you have to learn before you could go further. One useful way to bootstrap yourself is to only learn a minimal amount of basic ideas that are enough for you to “survive”  in the field. Such basic ideas are the Minimal Actionable Knowledge &amp; Experience (MAKE) of the field. Here, I will try to present the “MAKE” of the field of fast matrix computations.</p>

<!--more-->

<h3 id="low-rank-matrices-and-important-information">Low-rank matrices and important information</h3>

<p>An $m\times n$  matrix $\mathbf{A}$ is low-rank if its rank, $k\equiv\mathrm{rank}\,\mathbf{A}$, is far less than $m$ and $n$. Then $\mathbf{A}$ has a factorization <script type="math/tex">\mathbf{A} =\mathbf{E}\mathbf{F}</script> where $\mathbf{E}$ is a tall-skinny matrix with $k$ columns and $\mathbf{F}$ a short-fat matrix with $k$ rows.</p>

<p><img class="center" src="/images/blog_figures/lowRankFac.png" width="500"></p>

<p>For example the following $3\times3$ matrix is of rank-$1$ only.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{bmatrix}
1 & 2 & 3\\
1 & 2 & 3\\
1 & 2 & 3
\end{bmatrix}
=
\begin{bmatrix}
1\\
1\\
1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix} %]]&gt;</script>

<p>Given a matrix $\mathbf{A}$, there are many ways to find $\mathrm{rank}\,\mathbf{A}$. One way is to find the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a></p>

<script type="math/tex; mode=display">\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*</script>

<p>where $\mathbf{\Sigma}=\mathrm{diag}(\sigma_1,\sigma_2,\dots)$ is an $m\times n$ diagonal matrix, whose diagonal elements are called the singular values of $\mathbf{A}$. Then $\mathrm{rank}\,\mathbf{A}$ is the number of nonzero singular values.</p>

<p>The SVD tells you the most important information about a matrix: the <a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_spectral_norm)">Eckart-Young theorem</a> says that the best rank-$k$ approximation of $\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*$ can be obtained by only keeping the first $k$ singular values and zeroing out the rest in $\mathbf{\Sigma}$. When the singular values decay quickly, such a low-rank approximation can be very accurate. This is particularly important in practice when we want to solve problems efficiently by ignoring the unimportant information.</p>

<p>An interesting example is the $n\times n$ <a href="https://en.wikipedia.org/wiki/Hilbert_matrix">Hilbert matrix</a> $\mathbf{H}_n$, whose $(i,j)$ entry is defined to be $\frac{1}{i+j-1}$. $\mathbf{H}_n$ is full-rank for any size $n$, but it is <em>numerically low-rank</em>, meaning that its singular values decay rapidly such that given any small threshold $\epsilon$, only a few singular values are above $\epsilon$. For example with $\epsilon=10^{-15}$, the $1000\times1000$ has numerical rank $28$.</p>

<p>Other examples of (numerically) low-rank matrices include the <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde</a>, <a href="https://en.wikipedia.org/wiki/Cauchy_matrix">Cauchy</a>, <a href="https://en.wikipedia.org/wiki/Hankel_matrix">Hankel</a>, and <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz</a> matrices, as well as matrices constructed from smooth data or smooth functions.</p>

<p>As it turns out, a lot of the matrices we encounter in practice are numerically low-rank. So finding low-rank approximation (e.g. in the form of $\mathbf{A}=\mathbf{EF}$ at the beginning) is one of the most important and fundamental subjects in applied math nowadays.</p>

<h3 id="data-sparsity-and-rank-structured-matrices">Data sparsity and rank structured matrices</h3>

<p>Matrix sizes have been growing with technological advancements. Many common matrix algorithms scale cubically with the matrix size, meaning that even if your computing power grows 1000 times, you could only afford to solve problems that are 10 times bigger than before. These common algorithms include matrix multiplication, matrix inversion, and matrix factorizations (e.g. LU, QR, SVD). Therefore, it is important to speed up these matrix computation methods in order to fully exploit the ever growing computing power.</p>

<p>One major strategy to accelerate the computations is to exploit the <strong>data sparsity</strong> of a matrix. Data sparsity is a deliberately vague concept which broadly refers to the kind of internal structures in a matrix that can help make computations faster. Following are some common examples of data-sparse matrices.</p>

<ul>
  <li>The most classcial data-sparse matrices are the <strong>sparse matrices</strong>, ones with a large number of zero entries. Using <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">compressed data formats</a>, you can save a lot of memories by storing only the nonzero entries (together with their positions). You can greatly reduce computation time by only operating on the nonzero entries while maintaining the sparsity of the matrices (i.e., avoid introducing more nonzero entries). Common sparse matrices include diagonal/block-diagonal matrices, banded matrices, permutations, adjacency matrix of graphs, etc.</li>
  <li><strong>Low-rank matrices</strong>, as we have introduced above, are ones admit low-rank factorizations. Commonly used factorizations include the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs">reduced SVDs</a>, <a href="https://en.wikipedia.org/wiki/Interpolative_decomposition">interpolative decomposition</a>, <a href="https://en.wikipedia.org/wiki/CUR_matrix_approximation">CUR decomposition</a>, <a href="https://en.wikipedia.org/wiki/RRQR_factorization">rank-revealing QR factorizations</a>, etc. Normally the SVD algorithms are more expensive, so the other algorithms are preferred when possible; for very large matrices, all the factorization algorithms can be further accelerated by <a href="https://epubs.siam.org/doi/abs/10.1137/090771806">randomization techniques</a>.</li>
  <li><strong>Rank structured matrices.</strong> These matrices are not necessarily low-rank, but can be split into a relatively small number of submatrices, each of which is low-rank. For example, the picture below shows the structure of a <em>Hierarchically Off-diagonal Low Rank</em> (HODLR) matrix, where all the off-diagonal blocks, big or small, have similar ranks. Such structure can for example arise from gravitational or electrostatic interactions, where the diagonal blocks represent the local interactions and the off-diagonal blocks represent the far interactions; the far interactions are low-rank because they are much smoother than the local interactions. Other rank structured matrices include the <em>Hierarchically Semi-separable</em> (HSS) matrices, the inverse of banded matrices, and the more general <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix">$\mathcal{H}$-matrices</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix#H2-matrices">$\mathcal{H}^2$-matrices</a>. Rank structured matrices can be efficiently handled using tree structures. Matrix algorithms designed for these matrices can be very fast, with computation time scaling linearly or log-linearly with the matrix size.</li>
</ul>

<p><img class="center" src="/images/blog_figures/HODLR.png" width="400"></p>

<ul>
  <li><strong>Complementary low-rank matrices</strong> are a special type of rank structured matrices that can be decomposed by the <em>butterfly factorization</em> (BF). The BF is inspired by ideas of the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> algorithm (divide-and-conquer and permutations), which can be explained using the <a href="https://en.wikipedia.org/wiki/Butterfly_diagram">butterfly diagram</a>. Butterfly algorithms were initially motivated by solving oscillatory problems such as wave scattering.</li>
</ul>

<p>With these ideas above, plus a little coding experience with some simple rank structured matrices (a good place to start is with the first two of these <a href="https://amath.colorado.edu/faculty/martinss/2014_CBMS/codes.html">tutorial codes</a>), you are equipped with the “MAKE” that gets you ready for going on an advanture to the fast computations with matrices. All the details and other more advanced topics can be learned later once you dig far enough.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lockdown Math]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/12/lockdown-math/"/>
    <updated>2021-03-12T01:28:25-06:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/12/lockdown-math</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2021/03/04/your-one-big-desire">previous post</a> I talked about anxiety caused by things that you actually have control over. This time I’d like to also touch on situations that you can’t really change, such as a pandemic.</p>

<p>A pandemic makes people anxious because it freezes life. A lot of activities have to be suspended. You can’t do what you would normally do. You don’t know how long the pandemic would last — probably a few months, probably a few years. All you can do is wait, indefinitely.</p>

<h3 id="mathematicians-in-confinement">Mathematicians in confinement</h3>

<p>Some groups of people seem to be doing quite well in a lockdown situation. Mathematicians happen to be one such group.</p>

<!--more-->

<p>Sophus Lie, who established <a href="https://en.wikipedia.org/wiki/Lie_algebra">Lie algebra</a> during his imprisonment, said that “a mathematician is comparatively well suited to be in prison.” And Lie was not alone, 70 years later another great mathematician, André Weil, who also had a productive time in prison, wondered “if it’s only in prison that I work so well, will I have to arrange to spend two or three months locked up every year?”</p>

<p>According to the fun article <a href="http://www.nieuwarchief.nl/serie5/pdf/naw5-2020-21-2-095.pdf"><em>Lockdown Mathematics</em></a>, both Lie and Weil were mistaken for spies during wartimes “due to their strange habits as eccentric mathematicians who incessantly scribbled some sort of incomprehensible notes and wandered in nature without any credible purpose discernible to outsiders.”</p>

<p>There are many other interesting examples from that article. The bottom line is, many mathematicians have experienced highly productive periods during confinement situations, where they were free of distractions and could focus deeply on their thoughts; although in some cases, this effect only lasted for a month or two, eventually the productivity boost waned. After all, mathematicians are still humans who need some breaks.</p>

<p>I have to say that the above description perfectly summarized my experience during the COVID lockdown. I was super productive and wrote two papers during the first few months of the lockdown, then I started to get distracted and wanted to socialize again.</p>

<p>Now it is March again, an anniversary of the COVID lockdown in the US. Maybe I could also benefit from setting up a few months of faux lockdown for myself every year.</p>

<h3 id="flow-vs-pandemic">Flow v.s. Pandemic</h3>

<p>According to some <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242043">recent research</a>, getting in the state of <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow</a> might be one best way to cope with lockdown anxieties. When you are experiencing flow, you are deeply focused on something; time seems to slip by quickly, a few hours feel like just a moment to you.</p>

<p>Not just mathematicians, other people find their flow in all sorts of ways: painting, making handicrafts, reading books, writing essays, coding. So when you get into flow, not only that time passes much quicker, but you are also accomplishing something meaningful to you. It just completely flips the lockdown situation around and gives you a positive experience.</p>

<p>In fact, this experience may have deeper implications if we redefine “pandemic” as a state of mind:</p>

<blockquote>
  <p>A “pandemic” is an extended period during which you are constantly anxious about one thing that you can’t control.</p>
</blockquote>

<p>By this definition, people are constantly undergoing all kinds of pandemics: losing jobs, struggling to graduate, accumulating debts, being homesick, feeling lonely. In each of these cases, people are stuck in a “personal pandemic” that they can’t easily escape and have to live with it for an uncertain amount of time.</p>

<p>Inspired by the lockdown experience, maybe one solution to a “personal pandemic” is to accept it like we accepted that we would be living under COVID for a few years, and turn to focus on something we truly care about. Oftentimes, the consequence of a “personal pandemic” is not as dire as one might think it would be; a threat might also be an opportunity for growth. So being able to find your flow could carry you through the most difficult time of your “personal pandemic,” giving you the chance to come out stronger and better off.</p>
]]></content>
  </entry>
  
</feed>
