<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Math | Bobbie's Blog]]></title>
  <link href="http://bobbielf2.github.io/blog/categories/math/atom.xml" rel="self"/>
  <link href="http://bobbielf2.github.io/"/>
  <updated>2021-03-26T01:23:06-05:00</updated>
  <id>http://bobbielf2.github.io/</id>
  <author>
    <name><![CDATA[Bowei "Bobbie" Wu .]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[It Is Okay to Speed Up a Little]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little/"/>
    <updated>2021-03-26T00:57:22-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little</id>
    <content type="html"><![CDATA[<p>We naturally avoid making mistakes. If you were a caveman who survives by hunting, you’d try not to make mistakes that could get you killed by animals.</p>

<p>But in the modern days, we no longer worry about ferocious animals. We now have much safer spaces that are tolerant of our mistakes. So we can afford to make more mistakes. In fact, making mistakes is how we learn.</p>

<p>Yet, people avoid mistakes because it makes you “look bad.” But there is no learning without making mistakes. So how can we gain the courage to learn without worrying about the mistakes we would make?</p>

<p>I think math has the answer.</p>

<!--more-->

<p>In numerical analysis, we design algorithms that solve problems quickly and accurately. One major task when developing such an algorithm is to combat with numerical errors. If you open any classical numerical analysis textbook, you will likely see discussions of error analysis: how to eliminate or reduce roundoff errors, and how to prevent errors from propagating and contaminating the final results.</p>

<p>Talking about errors can sound bad and boring. It may drive beginners away from numerical analysis before they can learn the true beauty of the subject.</p>

<p>In fact, mathematicians know about the bad connotations associated with “errors.” When communicating with fellow mathematicians, we don’t talk about errors as often as one would imagine. For example, instead of saying “the algorithm makes smaller errors” we like to say “the algorithm converges faster.” And instead of “the algorithm prevents error propagation” we like to say “the algorithm is stable and robust.”</p>

<p>“Fast convergence” and “smaller errors” mean the same thing, but the framings are completely different. “Convergence” focuses on the positive effects and makes people excited. If you ask any numerical analyst what makes them love their field, I’ll bet you no one would say they love the error analysis; rather, they would tell you about the lightning computation speeds, the simple ideas behind powerful algorithms, the unexpected connections between ideas from different fields, and more.</p>

<p>Likewise, “making mistakes” gets its own bad rap. I propose replacing it with “speeding up” because making more mistakes allows you to learn quicker. Whenever you are stuck and can’t make any progress because you are afraid of making mistakes, try to tell yourself “I am just going to speed up my learning a little.”</p>

<p>Making mistakes can be embarrassing, but <a href="https://twitter.com/edlatimore/status/1058305553020141570">embarrassment is simply the cost of entry</a>. The embarrassment you feel only exist in your mind, learning is what can actually happen. Once you allow yourself to look stupid and take on the beginner’s mindset, learning will be unstoppable.</p>

<p>Make more mistakes. It is okay to speed up a little, and it feels great.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Low-rank and Rank Structured Matrices]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices/"/>
    <updated>2021-03-21T15:33:24-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices</id>
    <content type="html"><![CDATA[<p>When you begin to research a field of study, you often get overwhelmed by the amount of existing knowledge you have to learn before you could go further. One useful way to bootstrap yourself is to only learn a minimal amount of basic ideas that are enough for you to “survive”  in the field. Such basic ideas are the Minimal Actionable Knowledge &amp; Experience (MAKE) of the field. Here, I will try to present the “MAKE” of the field of fast matrix computations.</p>

<!--more-->

<h3 id="low-rank-matrices-and-important-information">Low-rank matrices and important information</h3>

<p>An $m\times n$  matrix $\mathbf{A}$ is low-rank if its rank, $k\equiv\mathrm{rank}\,\mathbf{A}$, is far less than $m$ and $n$. Then $\mathbf{A}$ has a factorization <script type="math/tex">\mathbf{A} =\mathbf{E}\mathbf{F}</script> where $\mathbf{E}$ is a tall-skinny matrix with $k$ columns and $\mathbf{F}$ a short-fat matrix with $k$ rows.</p>

<p><img class="center" src="/images/blog_figures/lowRankFac.png" width="500"></p>

<p>For example the following $3\times3$ matrix is of rank-$1$ only.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{bmatrix}
1 & 2 & 3\\
1 & 2 & 3\\
1 & 2 & 3
\end{bmatrix}
=
\begin{bmatrix}
1\\
1\\
1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix} %]]&gt;</script>

<p>Given a matrix $\mathbf{A}$, there are many ways to find $\mathrm{rank}\,\mathbf{A}$. One way is to find the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a></p>

<script type="math/tex; mode=display">\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*</script>

<p>where $\mathbf{\Sigma}=\mathrm{diag}(\sigma_1,\sigma_2,\dots)$ is an $m\times n$ diagonal matrix, whose diagonal elements are called the singular values of $\mathbf{A}$. Then $\mathrm{rank}\,\mathbf{A}$ is the number of nonzero singular values.</p>

<p>The SVD tells you the most important information about a matrix: the <a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_spectral_norm)">Eckart-Young theorem</a> says that the best rank-$k$ approximation of $\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*$ can be obtained by only keeping the first $k$ singular values and zeroing out the rest in $\mathbf{\Sigma}$. When the singular values decay quickly, such a low-rank approximation can be very accurate. This is particularly important in practice when we want to solve problems efficiently by ignoring the unimportant information.</p>

<p>An interesting example is the $n\times n$ <a href="https://en.wikipedia.org/wiki/Hilbert_matrix">Hilbert matrix</a> $\mathbf{H}_n$, whose $(i,j)$ entry is defined to be $\frac{1}{i+j-1}$. $\mathbf{H}_n$ is full-rank for any size $n$, but it is <em>numerically low-rank</em>, meaning that its singular values decay rapidly such that given any small threshold $\epsilon$, only a few singular values are above $\epsilon$. For example with $\epsilon=10^{-15}$, the $1000\times1000$ has numerical rank $28$.</p>

<p>Other examples of (numerically) low-rank matrices include the <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde</a>, <a href="https://en.wikipedia.org/wiki/Cauchy_matrix">Cauchy</a>, <a href="https://en.wikipedia.org/wiki/Hankel_matrix">Hankel</a>, and <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz</a> matrices, as well as matrices constructed from smooth data or smooth functions.</p>

<p>As it turns out, a lot of the matrices we encounter in practice are numerically low-rank. So finding low-rank approximation (e.g. in the form of $\mathbf{A}=\mathbf{EF}$ at the beginning) is one of the most important and fundamental subjects in applied math nowadays.</p>

<h3 id="data-sparsity-and-rank-structured-matrices">Data sparsity and rank structured matrices</h3>

<p>Matrix sizes have been growing with technological advancements. Many common matrix algorithms scale cubically with the matrix size, meaning that even if your computing power grows 1000 times, you could only afford to solve problems that are 10 times bigger than before. These common algorithms include matrix multiplication, matrix inversion, and matrix factorizations (e.g. LU, QR, SVD). Therefore, it is important to speed up these matrix computation methods in order to fully exploit the ever growing computing power.</p>

<p>One major strategy to accelerate the computations is to exploit the <strong>data sparsity</strong> of a matrix. Data sparsity is a deliberately vague concept which broadly refers to the kind of internal structures in a matrix that can help make computations faster. Following are some common examples of data-sparse matrices.</p>

<ul>
  <li>The most classcial data-sparse matrices are the <strong>sparse matrices</strong>, ones with a large number of zero entries. Using <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">compressed data formats</a>, you can save a lot of memories by storing only the nonzero entries (together with their positions). You can greatly reduce computation time by only operating on the nonzero entries while maintaining the sparsity of the matrices (i.e., avoid introducing more nonzero entries). Common sparse matrices include diagonal/block-diagonal matrices, banded matrices, permutations, adjacency matrix of graphs, etc.</li>
  <li><strong>Low-rank matrices</strong>, as we have introduced above, are ones admit low-rank factorizations. Commonly used factorizations include the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs">reduced SVDs</a>, <a href="https://en.wikipedia.org/wiki/Interpolative_decomposition">interpolative decomposition</a>, <a href="https://en.wikipedia.org/wiki/CUR_matrix_approximation">CUR decomposition</a>, <a href="https://en.wikipedia.org/wiki/RRQR_factorization">rank-revealing QR factorizations</a>, etc. Normally the SVD algorithms are more expensive, so the other algorithms are more practical; for very large matrices, all the factorization algorithms can be further accelerated by <a href="https://epubs.siam.org/doi/abs/10.1137/090771806">randomization techniques</a>.</li>
  <li><strong>Rank structured matrices.</strong> These matrices are not necessarily low-rank, but can be split into a relatively small number of submatrices, each of which is low-rank. For example, the picture below shows the structure of a <em>Hierarchically Off-diagonal Low Rank</em> (HODLR) matrix, where all the off-diagonal blocks, big or small, have similar ranks. Such structure can for example arise from gravitational or electrostatic interactions, where the diagonal blocks represent the local interactions and the off-diagonal blocks represent the far interactions; the far interactions are low-rank because they are much smoother than the local interactions. Other rank structured matrices include the <em>Hierarchically Semi-separable</em> (HSS) matrices, the inverse of banded matrices, and the more general <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix">$\mathcal{H}$-matrices</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix#H2-matrices">$\mathcal{H}^2$-matrices</a>. Rank structured matrices can be efficiently handled using tree structures. Matrix algorithms designed for these matrices can be very fast, with computation time scaling linearly or log-linearly with the matrix size.</li>
</ul>

<p><img class="center" src="/images/blog_figures/HODLR.png" width="400"></p>

<ul>
  <li><strong>Complementary low-rank matrices</strong> are a special type of rank structured matrices that can be decomposed by the <em>butterfly factorization</em> (BF). The BF is inspired by ideas of the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> algorithm (divide-and-conquer and permutations), which can be explained using the <a href="https://en.wikipedia.org/wiki/Butterfly_diagram">butterfly diagram</a>. Butterfly algorithms were initially motivated by solving oscillatory problems such as wave scattering.</li>
</ul>

<p>With these ideas above, plus a little coding experience with some simple rank structured matrices (a good place to start is with the first two of these <a href="https://amath.colorado.edu/faculty/martinss/2014_CBMS/codes.html">tutorial codes</a>), you are equipped with the “MAKE” that gets you ready for going on an advanture to the fast computations with matrices. All the details and other more advanced topics can be learned later once you dig far enough.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Lockdown Math]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/12/lockdown-math/"/>
    <updated>2021-03-12T01:28:25-06:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/12/lockdown-math</id>
    <content type="html"><![CDATA[<p>In the <a href="/blog/2021/03/04/your-one-big-desire">previous post</a> I talked about anxiety caused by things that you actually have control over. This time I’d like to also touch on situations that you can’t really change, such as a pandemic.</p>

<p>A pandemic makes people anxious because it freezes life. A lot of activities have to be suspended. You can’t do what you would normally do. You don’t know how long the pandemic would last — probably a few months, probably a few years. All you can do is wait, indefinitely.</p>

<h3 id="mathematicians-in-confinement">Mathematicians in confinement</h3>

<p>Some groups of people seem to be doing quite well in a lockdown situation. Mathematicians happen to be one such group.</p>

<!--more-->

<p>Sophus Lie, who established <a href="https://en.wikipedia.org/wiki/Lie_algebra">Lie algebra</a> during his imprisonment, said that “a mathematician is comparatively well suited to be in prison.” And Lie was not alone, 70 years later another great mathematician, André Weil, who also had a productive time in prison, wondered “if it’s only in prison that I work so well, will I have to arrange to spend two or three months locked up every year?”</p>

<p>According to the fun article <a href="http://www.nieuwarchief.nl/serie5/pdf/naw5-2020-21-2-095.pdf"><em>Lockdown Mathematics</em></a>, both Lie and Weil were mistaken for spies during wartimes “due to their strange habits as eccentric mathematicians who incessantly scribbled some sort of incomprehensible notes and wandered in nature without any credible purpose discernible to outsiders.”</p>

<p>There are many other interesting examples from that article. The bottom line is, many mathematicians have experienced highly productive periods during confinement situations, where they were free of distractions and could focus deeply on their thoughts; although in some cases, this effect only lasted for a month or two, eventually the productivity boost waned. After all, mathematicians are still humans who need some breaks.</p>

<p>I have to say that the above description perfectly summarized my experience during the COVID lockdown. I was super productive and wrote two papers during the first few months of the lockdown, then I started to get distracted and wanted to socialize again.</p>

<p>Now it is March again, an anniversary of the COVID lockdown in the US. Maybe I could also benefit from setting up a few months of faux lockdown for myself every year.</p>

<h3 id="flow-vs-pandemic">Flow v.s. Pandemic</h3>

<p>According to some <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0242043">recent research</a>, getting in the state of <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow</a> might be one best way to cope with lockdown anxieties. When you are experiencing flow, you are deeply focused on something; time seems to slip by quickly, a few hours feel like just a moment to you.</p>

<p>Not just mathematicians, other people find their flow in all sorts of ways: painting, making handicrafts, reading books, writing essays, coding. So when you get into flow, not only that time passes much quicker, but you are also accomplishing something meaningful to you. It just completely flips the lockdown situation around and gives you a positive experience.</p>

<p>In fact, this experience may have deeper implications if we redefine “pandemic” as a state of mind:</p>

<blockquote>
  <p>A “pandemic” is an extended period during which you are constantly anxious about one thing that you can’t control.</p>
</blockquote>

<p>By this definition, people are constantly undergoing all kinds of pandemics: losing jobs, struggling to graduate, accumulating debts, being homesick, feeling lonely. In each of these cases, people are stuck in a “personal pandemic” that they can’t easily escape and have to live with it for an uncertain amount of time.</p>

<p>Inspired by the lockdown experience, maybe one solution to a “personal pandemic” is to accept it like we accepted that we would be living under COVID for a few years, and turn to focus on something we truly care about. Oftentimes, the consequence of a “personal pandemic” is not as dire as one might think it would be; a threat might also be an opportunity for growth. So being able to find your flow could carry you through the most difficult time of your “personal pandemic,” giving you the chance to come out stronger and better off.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Underestimating My Ignorance]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/02/14/underestimating-my-ignorance/"/>
    <updated>2021-02-14T22:23:50-06:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/02/14/underestimating-my-ignorance</id>
    <content type="html"><![CDATA[<p>“Being ignorant is bad.” So would most people think. I try hard to be more knowledgeable and I am still ashamed of my own ignorance oftentimes. But I am also learning more and more about the positive power of ignorance — a power that has been underestimated.</p>

<!--more-->

<p>Thanks to last year’s COVID lockdown, I had the chance to concentrate on research without distractions. I was fortunate enough to make some discoveries, which I thought was a huge breakthrough. After submitting some papers, a few months later I found out that I have in fact overlooked some closely related work in the literature. One of my results looked less great given the existing work, although it was still a nice progress.</p>

<p>While this has been a humbling experience, it was also inspiring: I wouldn’t have been so optimistic if I knew how long a journey the pioneers have traveled; I might have lost my faith and given up early if I knew all the failed attempts by other people. It was exactly my ignorance that had given me the courage to attack the open problems and the hope to keep pushing. Luckily, I eventually bumped into paths and territories that others have overlooked which led to my destination.</p>

<p>As Alain Connes once wrote, the initial phase of making new math discoveries “requires a kind of protection of one’s ignorance.” Sometimes, ignorance “frees people from reverence for authority and allows them to rely on their intuition.” In the same spirit, Steve Jobs also told people to “stay hungry, stay foolish.” Perhaps all intellectuals, including academics and industrial innovators, can use some protections of ignorance.</p>

<p>We like to be well-prepared before going into a challenging adventure. But knowing all the failed journeys of other people, knowing that even some brave and strong peers had failed their missions, can actually paralyse you into inaction. In fact, exploring the math world doesn’t require your having “full knowledge” about any field. Nobody ever knew “enough.” Once you know a minimum amount of knowledge that allows you to survive, you can start your journey.</p>

<p>“Whatever the origin of one’s journey, one day, if one walks far enough, one is bound to stumble on a well-known town: for instance, elliptic functions, modular forms, or zeta functions.” This is another quote from Alain Connes, which resonates with me deeply. There is no single path to knowledge, we can be free to explore our own paths and not be ashamed of knowing too little about all other possible paths. Be brave and keep pushing, once in a while we will meet with other adventurers in one of those famous mathematical towns.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Sense of Potential Theory]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/02/06/making-sense-of-potential-theory/"/>
    <updated>2021-02-06T18:47:18-06:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/02/06/making-sense-of-potential-theory</id>
    <content type="html"><![CDATA[<p>Potential theory was developed a few centuries ago in part to solve the boundary value problems for partial differential equations (PDEs). It led to the so-called “indirect approach” to boundary integral equations for elliptic PDEs. The goal of this article is to give this indirect approach some physical meaning. Some basic knowledge of potential theory is assumed. (See the book by R. Kress<sup id="fnref:kress"><a href="#fn:kress" class="footnote">1</a></sup> for more details.)</p>

<p>Consider the classical Laplace equation in a domain $\Omega\subset\mathbb{R}^2$, with Dirichlet boundary condition on the boundary $\Gamma:=\partial\Omega$, written as</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{aligned}
\Delta u(x) &= 0 & x\in\Omega\\
u(x) &= f(x) & x\in\Gamma
\end{aligned} %]]&gt;</script>

<p>The theory of integral equations solves a given boundary value problem like this by reformulating it into an integral equation on the boundary of the domain, such that the 2D <em>spatial differential problem</em> in $\Omega$ is reduced to a 1D <em>boundary integral problem</em> on $\Gamma$.</p>

<p>There are two main approaches to integral equations: the <em>direct approach</em> and the <em>indirect approach</em>.</p>

<!--more-->

<p><strong>1.  Green’s representation (direct approach).</strong> The Green’s representation theorem says that the unknown function $u(x)$ can be expressed as</p>

<script type="math/tex; mode=display">u(x) = \int_\Gamma \Big(G(x,y)\frac{\partial u(y)}{\partial n_y} - \frac{\partial G(x,y)}{\partial n_y}u(y)\Big)\,ds_y,\quad x\in\Omega</script>

<p>where</p>

<script type="math/tex; mode=display">G(x,y)=\frac{1}{2\pi}\log\frac{1}{|x-y|}</script>

<p>is the fundamental solution of the Laplace equation. Then by letting $x\to\Gamma$, this representation becomes an integral equation</p>

<script type="math/tex; mode=display">\int_\Gamma G(x,y)\psi(y)\,ds_y = \int_\Gamma\frac{\partial G(x,y)}{\partial n_y}f(y)\,ds_y + \frac{f(x)}{2},\quad x\in\Gamma</script>

<p>where the function $\psi(y) = \frac{\partial u(y)}{\partial n_y}$ is the unknown Neumann data on $\Gamma$, and the $f(x)/2$ term is due to the so-called jump relation<sup id="fnref:kress:1"><a href="#fn:kress" class="footnote">1</a></sup>.</p>

<p><strong>2. Potential theory (indirect approach).</strong> In potential theory, one starts by assuming that the solution has the form</p>

<script type="math/tex; mode=display">u(x) = \int_\Gamma \frac{\partial G(x,y)}{\partial n_y}\varphi(y)\,ds_y,\quad x\in\Omega</script>

<p>where $\varphi$ is a “density function” on $\Gamma$. Then again by letting $x\to\Gamma$ and using the jump relation, we arrive at the integral equation</p>

<script type="math/tex; mode=display">\int_\Gamma \frac{\partial G(x,y)}{\partial n_y}\varphi(y)\,ds_y-\frac{\varphi(x)}{2}=f(x),\quad x\in\Gamma</script>

<p>Comparing these two approaches, we see that the unknown function $\psi=\frac{\partial u}{\partial n}$ in the Green’s representation approach has clear physical meaning (e.g.,  if $u$ is the temperature, then $\psi$ is the heat flux at the boundary), hence the name “direct approach.” On the other hand, the unknown function $\varphi$ in the potential theory approach doesn’t have a direct physical meaning, therefore the name “indirect approach.”</p>

<p>In fact, there are two ways to make sense of this density function $\varphi$.</p>

<p><strong>1. The charge density analogy.</strong> The name “potential theory” comes from the fact that the Laplace equation describes the gravitational potential or electrostatic potential in space. If $G(x,y)$ represents the electric potential generated by a point charge at $y$, then $\frac{\partial G(x,y)}{\partial n_y}$ is the potential generated by a dipole charge at $y$, hence $\varphi$ is the dipole charge density on $\Gamma$. Potential theory then generalizes the concept of charge density to other elliptic PDEs as well, terming $\varphi$ the density function for a variety of potentials. (E.g., velocity potential, traction potential, electromagnetic potential, etc.)</p>

<p><strong>2. The jump of physical quantities.</strong> Another way to give meaning to $\varphi$ is to go through the process of how we arrived at an assumption such as $u(x) = \int_\Gamma \frac{\partial G(x,y)}{\partial n_y}\varphi(y)\,ds_y$. The key fact is that, with such an assumption, one is actually extending the solution $u(x)$ from $\Omega$ to the entire space $\mathbb{R}^2$ based on an underlying continuity assumption on $u$. Specifically, let’s assume a solution $U(x)$ for all $x\in\mathbb{R}^2\setminus\Gamma$, such that</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
U(x) = \begin{cases}
u(x) & x\in\Omega\\
u_\mathrm{out}(x) & x\in\mathbb{R}^2\setminus\bar\Omega
\end{cases} %]]&gt;</script>

<p>i.e., $U$ is an extension of $u$ into the whole space by stitching together the field $u$ inside $\Omega$ and some unknown harmonic field $u_\mathrm{out}$ outside $\Omega$ . According to the interior and exterior versions of Green’s representation theorem, we have</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\int_\Gamma \Big(G(x,y)\frac{\partial u(y)}{\partial n_y} - \frac{\partial G(x,y)}{\partial n_y}u(y)\Big)\,ds_y =
\begin{cases}
u(x), & x\in\Omega\\
0, & x\in\mathbb{R}^2\setminus\bar\Omega
\end{cases} %]]&gt;</script>

<p>and</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\int_\Gamma \Big(\frac{\partial G(x,y)}{\partial n_y}u_\mathrm{out}(y) - G(x,y)\frac{\partial u_\mathrm{out}(y)}{\partial n_y}\Big)\,ds_y =
\begin{cases}
-u_\infty, & x\in\Omega\\
u_\mathrm{out}(x)-u_\infty, & x\in\mathbb{R}^2\setminus\bar\Omega
\end{cases} %]]&gt;</script>

<p>where $u_\infty$ is a constant associated to $u_\mathrm{out}$ at $\infty$. Without loss of generality, let’s just assume $u_\infty=0$ and add the above two expressions together, this yields</p>

<script type="math/tex; mode=display">U(x) = \int_\Gamma G(x,y)\Big(\frac{\partial u(y)}{\partial n_y}-\frac{\partial u_\mathrm{out}(y)}{\partial n_y}\Big)\,ds_y - \int_\Gamma\frac{\partial G(x,y)}{\partial n_y}\big(u(y)-u_\mathrm{out}(y)\big)\,ds_y.</script>

<p>If we assume that the normal derivative of $U$ is continuous across the boundary $\Gamma$, i.e. the normal derivative of $u$ matches that of $u_\mathrm{out}$ on $\Gamma$, then the first integral in the above representation vanishes. Therefore, defining the density $\varphi$ as</p>

<script type="math/tex; mode=display">\varphi(y) := u_\mathrm{out}(y) - u(y),\quad y\in\Gamma,</script>

<p>we recover the potential theoretic representation $u(x) = \int_\Gamma \frac{\partial G(x,y)}{\partial n_y}\varphi(y)\,ds_y$ for $x\in\Omega$.</p>

<p>In summary, the density function $\varphi$ physically represents the jump of the extended field $U(x)$ across the boundary $\Gamma$, assuming its normal derivative is continuous across $\Gamma$.</p>

<p>Based on the above idea of field-extension, we consequently obtain an intuitive picture about the solvability relations between the interior and exterior boundary value problems:</p>

<p><strong>1)</strong> When using potential theory to solve the exterior Neumann problem, we assume that the extended field $U(x)$ has matching Dirichlet data across the boundary, and then solve for the unknown jump of the Neumann data $\psi(y):=\frac{\partial u(y)}{\partial n_y}-\frac{\partial u_\mathrm{out}(y)}{\partial n_y}$. Since we know that the interior Dirichlet problem is uniquely solvable, the exterior Neumann problem is also uniquely solvable (with some appropriate behavior at the infinity).</p>

<p><strong>2)</strong> Likewise, the exterior Dirichlet problem is solved by matching the Neumann data of the extended field $U(x)$, and then solve for the jump of Dirichlet data across $\Gamma$.  Because the solution of an interior Neumann problem is only unique up to an arbitrary constant, the naive potential assumption for the exterior field $u_\mathrm{out}(x) = \int_\Gamma \frac{\partial G(x,y)}{\partial n_y}\varphi(y)\,ds_y$ will result in an integral equation with a one-dimensional nullspace. An additional condition is needed, besides the potential theoretic equation, in order to retrieve the unique-solvability of the exterior Dirichlet problem using integral equations.</p>

<div class="footnotes">
  <ol>
    <li id="fn:kress">
      <p>Kress, R. (2013). <em>Linear Integral Equations</em> (Vol. 82). Springer Science &amp; Business Media. (Chapter 6: Potential Theory) <a href="#fnref:kress" class="reversefootnote">&#8617;</a> <a href="#fnref:kress:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
</feed>
