<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Yq | Bobbie's Blog]]></title>
  <link href="http://bobbielf2.github.io/blog/categories/yq/atom.xml" rel="self"/>
  <link href="http://bobbielf2.github.io/"/>
  <updated>2021-04-15T20:07:19-05:00</updated>
  <id>http://bobbielf2.github.io/</id>
  <author>
    <name><![CDATA[Bowei "Bobbie" Wu .]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Year-long Writing Challenge]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/14/year-long-writing-challenge/"/>
    <updated>2021-04-14T19:15:23-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/14/year-long-writing-challenge</id>
    <content type="html"><![CDATA[<p>As a “365-day challenge,” I have been doing exercise every day for at least 15 minutes a day since last December, and will continue to do so throughout this year. I have to admit that this feels almost too easy.<sup id="fnref:tough"><a href="#fn:tough" class="footnote">1</a></sup> This is why I am in fact doing another challenge at the same time: to publish at least 40 blog posts this year.</p>

<p>Of course, I won’t publish anything of bad quality by my standard. So this challenge will definitely stretch me to learn and think more regularly, so that I can create something I am happy with every week or so.</p>

<p>On the other hand, publishing 40 articles doesn’t feel overwhelming, which leaves me enough room to work on research projects. A few years ago, I tried blogging on WeChat by setting too ambitious goals. I eventually stopped posting completely after a little more than 10 months; nevertheless, it was a rewarding experience.</p>

<p>With that, I am writing again this year. So far I have been enjoying it and have learned many new things already. Hopefully I won’t blow up my creative engine this time.</p>

<!--more-->

<h3 id="why-write">Why write?</h3>

<p>For a project that lasts as long as a year (or even just a few months), I never rely on willpower to do it. I know myself well enough that I know willpower can only carry me for a few days at best. So I look for motivations that can carry me for as long as I want.</p>

<p>For this year-long writing and publishing challenge, I have had a variety of reasons and desires to do it. I like that when I have many reasons to do something, I will be motivated by at least a few of them on any given day.</p>

<p>Below, I list my reasons to write. Maybe some of them will resonate with somebody who is (considering) writing regularly.</p>

<p><strong>1 Writing changes your life in many different ways.</strong> This is my general feeling about writing, purely emotional and based on my own experience. Twice in my life I had been transformed by writing, the first time during high school and the second time three years ago blogging on WeChat. I used to believe that why you do something should be based on logic, but not anymore. Oftentimes, your emotion and desire are more important – logic can’t explain why we pick certain axioms as the starting point for a mathematical theory, but experience and taste can.</p>

<p><strong>2 Writing distills your existing ideas and make rooms for the new ones.</strong> Every time I have published an article, I feel like I am saving my organized thoughts to a hard drive, so that my brain has more memories to welcome more new ideas. For example, writing “<a href="/blog/2021/04/04/what-is-operational-calculus/"><em>What Is Operational Calculus?</em></a>” had really given me a new insight into the Euler-Maclaurin formula and prepared me to explore its deeper connections with other fields.<sup id="fnref:write-speak"><a href="#fn:write-speak" class="footnote">2</a></sup></p>

<p><strong>3 Writing gives you a more memorable life.</strong> Do you feel that time slips by so quickly when every day is the same? Writing an essay is like going on an adventure, you will end up changing a lot of your thoughts and discovering ideas you had never expected. When you keep adventuring to different places, at the end of the year you will have so much colorful experience and so many wonderful memories that it feels like you are living a longer life. For example, 2020 might have passed too quick for many people, but it was <a href="/blog/2021/03/12/lockdown-math/">a quality year for me</a>.</p>

<p><strong>4 Writing shifts your focus from <em>problem-solving</em> to <em>understanding</em></strong> because in order to explain something to someone you have to first understand it. Deep understanding is what distinguishes a great mathematician from an okay mathematician.<sup id="fnref:erdos-grothendieck"><a href="#fn:erdos-grothendieck" class="footnote">3</a></sup></p>

<p><strong>5 Writing is embedded in academic lives anyways, why not publish them?</strong> We write research papers, teaching plans, proposals, presentation slides, reviewer reports, email responses. Most of the things we write are unpublished. However, turning some of these writings into publishable articles have generated new ideas for myself. For example, I wrote <a href="/blog/2021/01/30/audience-under-the-lightcone/"><em>Audience Under the Lightcone</em></a> and <a href="/blog/2021/02/14/underestimating-my-ignorance/"><em>Underestimating My Ignorance</em></a> both during my preparations for giving research talks.<sup id="fnref:tao"><a href="#fn:tao" class="footnote">4</a></sup></p>

<p><strong>6 Writing marries different ideas to give birth to new ideas.</strong> For example, <a href="/blog/2021/04/10/questioning-history/"><em>Questioning History</em></a> was a great inspiration to me because it combined education and understanding, two themes I loved to think about, and put them under the framework of history study, which was new to me.</p>

<p><strong>7 Writing absorbs you into a mind-bending field.</strong> Once you start writing, you are transported into an imaginary playground. You will wake up after a writing session realizing that a few hours have passed. It is almost similar to scrolling on social media for a few hours without realizing it; the only difference is that, after a writing session you feel a sense of reward instead of emptyness.</p>

<p><strong>8 Writing regularly gives structure to life.</strong> Doing research alone is not enough for <a href="/blog/2021/01/10/y-combinators-startup-advice-an-academic-perspective/">building an academic career</a>. When I am working on research problems, my mind is often too occupied to think of anything else. Writing regularly gives me the opportunity to ponder on other aspects of my career, such as teaching, networking, and exploring new interests.</p>

<p><strong>9 Writing builds you a radar for detecting good ideas.</strong> Quality input is indispensable to produce quality output. I built the habit of taking notes and jotting down thoughts anytime, so that I don’t lose any sparks that can be turned into an article later. Many of the articles I have written started as quick notes on simple ideas, for example the ideas in <a href="/blog/2021/03/04/your-one-big-desire/"><em>Your One Big Desire</em></a> and <a href="/blog/2021/03/26/it-is-okay-to-speed-up-a-little/"><em>It Is Okay to Speed Up a Little</em></a> both came up when I was focused on unrelated things.</p>

<p>I have more reasons to write than I can list them all here. As I wrote down the list above, more thoughts and inspirations were springing to my mind, I had to put some of them into footnotes so that I didn’t digress. As always, I end up publishing an article with almost no sentence that I originally planned to write. What a journey!</p>

<p>Stay tuned and I will have more to share as I dive into the next adventures.</p>

<div class="footnotes">
  <ol>
    <li id="fn:tough">
      <p>Of course, I have had a few tougher days, such as the <a href="/blog/2021/01/02/making-small-changes-in-life/">rainy and snowy days</a> and the <a href="/blog/2021/02/22/one-disastrous-week-in-austin/">one disastrous week in Austin</a>; I managed to still exercise on those days. There will be some upcoming challenges as well, like the days after when I will be getting my second dose of COVID vaccine, and days when I will be driving across states. I will see how things go. <a href="#fnref:tough" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:write-speak">
      <p>Writing allows you to discover good ideas because you can spend as much time on each sentence as you want and rewrite as many times as you like. Graham explains this point very well in <a href="http://www.paulgraham.com/speak.html"><em>Writing and Speaking</em></a>. <a href="#fnref:write-speak" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:erdos-grothendieck">
      <p>Regarding <em>problem-solving</em> vs <em>understanding</em> in mathematics, I think it is necessary to develop good problem-solving skills, but eventually deep understanding is what is required to do great math. I came across <a href="https://mathoverflow.net/a/7156/110149">this funny quote</a> that expresses exactly this point: “An excellent Erdös might reach certain limits as a mathematician, while a bad Erdös can still be an okay mathematician. On the other hand, a good Grothendieck can be a truly great mathematician, while a bad Grothendieck is really terrible.” <a href="#fnref:erdos-grothendieck" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:tao">
      <p>I think Terry Tao apparently uses the same strategy for blog posting. For example, he must have answered a lot of questions from students and colleagues about mathematical writing, so he had summarized his thoughts on this topic by posting the <a href="https://terrytao.wordpress.com/advice-on-writing-papers/"><em>On writing</em></a> series. <a href="#fnref:tao" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Questioning History]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/10/questioning-history/"/>
    <updated>2021-04-10T01:30:19-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/10/questioning-history</id>
    <content type="html"><![CDATA[<p>Lately, I have been listening to this <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast series</a> that has completely changed how I see the history of science.</p>

<p>In the series, Viktor Blåsjö, a professor and an emerging historian of mathematics, told a mind-blowing story about Galileo which is nothing like what we are usually told about. We have been told by the teachers, by the media, and by the mainstream science philosophers, that Galileo was the “father of modern science.” But to the contrary, Galileo was not only poor in mathematics, but also a dilettante physicist. Let me name a few things from the podcast series and you will know how interesting it is:</p>

<!--more-->

<ul>
  <li>Galileo couldn’t even solve some easier math problems, let alone understanding the new developments in math and physics of his contemporary mathematicians. That was why he tried to gain fame by attacking Aristotle from 2000 years ago, like beating a dead horse.</li>
  <li>Galileo picked on Aristotle also because he could not understand Archimedes, the true master of math and physics from Aristotle’s time. Framing Aristotle as the great authority of physics makes it easy for Galileo to defeat. Had he picked on Archimedes, his whole arguments would just fall apart.</li>
  <li>Contrary to the common belief that Galileo took experiements and obervations seriously, he often manipulated the data just to prove his points, made baseless and wrong guesses, and even plagiarized results from other people.</li>
  <li>Why haven’t the mainstream philosophers and historians debunked Galileo already? Because they also lack the skill to appreciate the great work of the mathematicians since Archimedes. Nowadays, <a href="https://youtu.be/LbbQ8SaYy9M?t=702">20 times more</a> papers about Aristotle than about Archimedes are being published under the category of history of science.</li>
</ul>

<p>Of course, all of these sound like wild claims if you are hearing them for the first time. So I would encourage any curious minds to go listen to the <a href="http://intellectualmathematics.com/blog/the-case-against-galileo-s01-overview/">podcast</a> or read the corresponding <a href="https://arxiv.org/abs/2102.06595">monograph</a> then judge for themselves.</p>

<p>This new interpretation of Galileo has solved one of my long-standing puzzles for me. In all the serious science textbooks, the most important discoveries are often associated with great mathematicians (such as the principles and laws associated with <a href="https://en.wikipedia.org/wiki/Archimedes%27_principle">Archimedes</a>, <a href="https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion">Kepler</a>, <a href="https://en.wikipedia.org/wiki/Newton%27s_law_of_universal_gravitation">Newton</a>, <a href="https://en.wikipedia.org/wiki/Bernoulli%27s_principle">Bernoulli</a>, <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation">Euler</a>, <a href="https://en.wikipedia.org/wiki/Gauss%27s_law">Gauss</a>). Why then has Galileo, “the father of modern science,” never been linked to any of those physical laws, other than the folklore of the “Pisa experiment”? Because he lacked the mathematical ability to do serious physics.</p>

<p>Studying the history of mathematics allows us to understand the more true history of science. But there are more reasons to study the history.</p>

<h3 id="why-study-history-of-mathematics">Why study history of mathematics?</h3>

<p>We know that Leibniz was one of the inventors of calculus. But do you know that Leibniz did not actually prove the fundamental theorem of calculus? In fact, he did not worry much about the foundation of calculus, certain not the “rigorous definition” of derivatives.</p>

<p>What Leibniz cared most about was the <em>transcendental curves</em>, that is, the graphs of transcendental functions. According to the inspiring book <a href="https://www.sciencedirect.com/book/9780128132371"><em>Transcendental Curves in the Leibnizian Calculus</em></a>, again by Blåsjö, “the problem of transcendental curves was to him (Leibniz) the guiding star for the better part of his mathematical works throughout his life.”</p>

<p>To Leibniz, functions like $\log(x)$ and $\cosh(x)$ are nothing but notations; the claim that $y=\cosh(x)$ solves the equation $y’'=y$ makes no sense. The important thing to Leibniz was that someone can actually graph a function like $\log(x)$, or calculate its value given any input $x$. After all, what’s the point of writing the symbol “$\log(x)$,” or naming a function “hyperbolic,” if you can’t even find their values? Therefore, to Leibniz, an expression like $\int_1^x \frac{1}{t}\,dt$ makes much more sense than $\log(x)$; being able to draw a <a href="https://en.wikipedia.org/wiki/Catenary">catenary</a> by hanging a chain is more important than simply writing $\cosh(x)=(e^x+e^{-x})/2$.</p>

<p>The same can be said about the other common transcendental functions such as $\sin(x)$ and $\cos(x)$. Indeed, students are being told that sinusoidal functions are “elementary functions,” but how “elementary” is it if you can’t even tell what $\sin(1)$ is right away? People still love to see animations, such as <a href="https://twitter.com/LucasVB/status/1378529237322334208?s=20">this one</a> and <a href="https://twitter.com/mathladyhazel/status/1348876985053958147?s=20">this one</a>, that can help them understand what trigonometric functions are.</p>

<p>The discussions above give us another important reason to study the history of mathematics. History tells us what are the “natural” things to pay attention to when we first teach or learn a mathematical concept. Knowing why we need transendental functions in the first place is <em>understanding</em>; telling people that these functions are elementary is <em>indoctrination</em>.</p>

<p>True education is about understanding. Unfortunately, a lot of the schooling nowadays are about indoctrination (and probably day-care as well).</p>

<h3 id="right-way-to-study-history">Right way to study history</h3>

<p>In Blåsjö’s <a href="http://intellectualmathematics.com/history-of-mathematics/">History of Mathematics</a> course, he contrasted <a href="https://youtu.be/SX0SpjAJJ9M">two ways of studying history of thought</a>:</p>

<ul>
  <li><strong>The bad way</strong>: to enforce our own beliefs and ways-of-thinking on the people in the past. For example, a bad historian might ask:
    <ul>
      <li>Who first discover this thing that I believe in?</li>
      <li>When did people start thinking like us?</li>
    </ul>
  </li>
  <li><strong>The good way</strong>: to embrace the diversity of thoughts, be curious about different perspectives. For example, a good historian might ask:
    <ul>
      <li>Why do people used to think differently?</li>
      <li>Why did their way of thinking make more sense at their time?</li>
    </ul>
  </li>
</ul>

<p>Historians who enforce their own beliefs on people from the past cannot find out the true history of Galileo, let alone the true history of science. Teachers who enforce their own thoughts on students cannot inspire anyone to understand.</p>

<p>Next time you are told that somethings are conventional or have been the way they are, just remember <a href="https://twitter.com/AmuseChimp/status/906147488582787073?s=20">this powerful quote from an amused chimp</a>:</p>

<blockquote>
  <p>If the news are fake, imagine history.</p>
</blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What Is Operational Calculus? Finite Differences and the Euler-Maclaurin Formula]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus/"/>
    <updated>2021-04-04T23:45:42-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/04/04/what-is-operational-calculus</id>
    <content type="html"><![CDATA[<p>When we first learned ordinary differential equations (ODEs) in class, we were told that the solution of</p>

<script type="math/tex; mode=display">f'(x)=f(x)</script>

<p>is $f(x)=Ce^x$ for an arbitrary constant $C$. We were told that this solution can be obtained either by observation and guessing (then it’s easy to verify solution), or by a separation of variable (e.g. integrate $dy/y=dx$). But these approaches don’t tell us what to do for higher-order equations, such as $f’'-3f’+2f=0$.</p>

<p>The <strong>operational calculus</strong> provides a convenient way to algebraically derive the solution to these linear ODEs. The derivation goes as follows.</p>

<!--more-->

<ul>
  <li>We want to solve the ODE $f’(x)=f(x)$, or equivalently $(\frac{d}{dx}-1)f(x)=0$.</li>
  <li>Define the differential operator $D:=\frac{d}{dx}$, then the equation becomes $(D-1)f(x)=0$.</li>
  <li>Define the integral operator $J$ such that $Jf(x):=\int_0^xf(t)dt$. (Therefore by the fundamental theorem of calculus, for any smooth function $g(x)$ we have $DJg(x)= g(x)$ and $JDg(x)=g(x)+C$, where the constant $C=-g(0)$. In particular, if $g(0)=0$ then we have $DJg(x)=JDg(x)=g(x)$, so we can write $J=D^{-1}$.)</li>
  <li>Rewrite the ODE as $\frac{1-D^{-1}}{D^{-1}}f(x)=0$, so $f(x)=\frac{D^{-1}}{1-D^{-1}}0 = \frac{1}{1-J}(D^{-1}0)$</li>
  <li>Suppose the above $0$ function is the derivative of some constant $C$, that is, $D^{-1}0=C$, then $f(x)=\frac{1}{1-J}C=C\left(\frac{1}{1-J}1\right)$</li>
  <li>By the Taylor series we have $\frac{1}{1-J}=1 + J+J^2+\dots=\sum_{n=0}^\infty J^{n}$, we can write $f(x)=C\sum_{n=0}^\infty J^{n}1$</li>
  <li>By induction, $J1=\int_0^x1\,dt=x,\; J^21=\int_0^xt\,dt=\frac{x^2}{2},\dots,J^n1=\frac{x^n}{n!}$. Therefore $f(x)=C\sum_{n=0}^\infty \frac{x^n}{n!}=C e^x$.</li>
</ul>

<p>Similarly we can show that $(D-a)f(x)=0$ has a solution $f(x)=Ce^{ax}$. Then for a higher-order equation such as $f’'-3f’+2f=0$, we can rewrite it as $(D^2-3D+2)f=0$ which can be then factored into $(D-1)(D-2)f(x)=0$, implying either $(D-1)f=0$ or $(D-2)f=0$.</p>

<p>Operational calculus reduces a differential equation into an algebraic equation, which is often much easier to solve.</p>

<h3 id="the-calculus-of-finite-differences">The calculus of finite differences</h3>

<p>The calculus of finite differences is a brilliant extension of the operational calculus to discrete quantities. The most important observation is that, given the <em>shift operator</em></p>

<script type="math/tex; mode=display">Ef(x):=f(x+h),</script>

<p>there is this remarkable <em>exponential map</em>, $E=\exp(hD)$, that connects the continuous and discrete worlds. This exponential map is simply a different way of writing the Taylor series expansion:</p>

<script type="math/tex; mode=display">Ef(x)=\exp(hD)f(x)=\left(1+hD+\frac{(hD)^2}{2!}+\frac{(hD)^3}{3!}+\dots\right)f(x)</script>

<p>From here, a lot of the great things can happen. For example, if we want to derive a <em>forward difference</em> formula such as</p>

<script type="math/tex; mode=display">f'(x)\approx \frac{f(x+h)-f(x)}{h},</script>

<p>we can define the forward difference operator</p>

<script type="math/tex; mode=display">\Delta f(x):=f(x+h)-f(x)=(E-1)f(x)</script>

<p>and write down the relationship</p>

<script type="math/tex; mode=display">\Delta+1=E=\exp(hD).</script>

<p>Then it follows naturally that</p>

<script type="math/tex; mode=display">D=\frac{1}{h}\log(1+\Delta)=\frac{1}{h}\left(\Delta - \frac{\Delta^2}{2} + \frac{\Delta^3}{3} - \dots\right)</script>

<p>Truncating this series at the first term gives back the first-order difference formula $f’(x)\approx\frac{1}{h}\Delta f(x)$ as before; truncating at the second term gives the second-order formula</p>

<script type="math/tex; mode=display">f'(x)\approx\frac{1}{h}(\Delta-\Delta^2/2)f(x)=\frac{1}{h}\left(-\frac{1}{2}f(x+2h)+2f(x+h)−\frac{3}{2}f(x)\right)</script>

<p>and so on.</p>

<h3 id="the-euler-maclaurin-formula">The Euler-Maclaurin formula</h3>

<p>Another striking application of such finite difference calculus is to derive the more advanced <a href="https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula"><em>Euler-Maclaurin formula</em></a>, which is in some sense a discrete version of the fundamental theorem of calculus.</p>

<p>As we have mentioned in the first part of this article, the fundamental theorem of calculus can be written in terms of operators as</p>

<script type="math/tex; mode=display">D^{-1}f(x)=Jf(x)+C=\int_0^xf(t)\,dt+C</script>

<p>So we would like to know if we replace the differential operator $D$ with the difference operator $\Delta$, how would this relationship change? What is $\Delta^{-1}f(x)$?</p>

<p>We have shown previously that $1+\Delta=\exp(hD)$, so it would be naturaly to write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{1}{\exp(hD)-1}.</script>

<p>However, the above right-hand side corresponds to the function $1/(e^x-1)$ which is singular at the origin, so instead we write</p>

<script type="math/tex; mode=display">\Delta^{-1}=\frac{hD}{\exp(hD)-1}(hD)^{-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}(hD)^{n}(hD)^{-1}.</script>

<p>Here we have used the fact that</p>

<script type="math/tex; mode=display">\frac{x}{e^x-1}=\sum_{n=0}^{\infty}\frac{B_n}{n!}x^n</script>

<p>where $B_n$ are the <a href="https://en.wikipedia.org/wiki/Bernoulli_numbers">Bernoulli numbers</a>. Notice that with our notations, we have</p>

<script type="math/tex; mode=display">(hD)^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C</script>

<p>and</p>

<script type="math/tex; mode=display">(hD)^{n}(hD)^{-1}f(x)=h^{n-1}f^{(n-1)}(x)</script>

<p>therefore</p>

<script type="math/tex; mode=display">\Delta^{-1}f(x)=\frac{1}{h}\int_0^xf(t)\,dt+C+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}f^{(n-1)}(x)</script>

<p>Now apply $\Delta$ on both sides, we have</p>

<script type="math/tex; mode=display">f(x)=\frac{1}{h}\int_x^{x+h}f(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}(f^{(n-1)}(x+h)-f^{(n-1)}(x))</script>

<p>This is one version of the Euler-Maclaurin formula. Finally, if we replace $x$ by $a,\;a+h,\;a+2h,\;\dots,\;a+(m-1)h$, one-by-one, and sum all the resulting formulae together, we get</p>

<script type="math/tex; mode=display">\sum_{k=0}^{m-1}f(a+kh)=\frac{1}{h}\int_a^bf(t)\,dt+\sum_{n=1}^{\infty}\frac{B_n}{n!}h^{n-1}\left(f^{(n-1)}(b)-f^{(n-1)}(a)\right)</script>

<p>where $b:=a+mh$. So here we are, this gives us the usual form of the Euler-Maclaurin formula.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[It Is Okay to Speed Up a Little]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little/"/>
    <updated>2021-03-26T00:57:22-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/26/it-is-okay-to-speed-up-a-little</id>
    <content type="html"><![CDATA[<p>We naturally avoid making mistakes. If you were a caveman who survives by hunting, you’d try not to make mistakes that could get you killed by animals.</p>

<p>But in the modern days, we no longer worry about ferocious animals. We now have much safer spaces that are tolerant of our mistakes. So we can afford to make more mistakes. In fact, making mistakes is how we learn.</p>

<p>Yet, people avoid mistakes because it makes you “look bad.” But there is no learning without making mistakes. So how can we gain the courage to learn without worrying about the mistakes we would make?</p>

<p>I think math has the answer.</p>

<!--more-->

<p>In numerical analysis, we design algorithms that solve problems quickly and accurately. One major task when developing such an algorithm is to combat with numerical errors. If you open any classical numerical analysis textbook, you will likely see discussions of error analysis: how to eliminate or reduce roundoff errors, and how to prevent errors from propagating and contaminating the final results.</p>

<p>Talking about errors can sound bad and boring. It may drive beginners away from numerical analysis before they can learn the true beauty of the subject.</p>

<p>In fact, mathematicians know about the bad connotations associated with “errors.” When communicating with fellow mathematicians, we don’t talk about errors as often as one would imagine. For example, instead of saying “the algorithm makes smaller errors” we like to say “the algorithm converges faster.” And instead of “the algorithm prevents error propagation” we like to say “the algorithm is stable and robust.”</p>

<p>“Fast convergence” and “smaller errors” mean the same thing, but the framings are completely different. “Convergence” focuses on the positive effects and makes people excited. If you ask any numerical analyst what makes them love their field, I’ll bet you no one would say they love the error analysis; rather, they would tell you about the lightning computation speeds, the simple ideas behind powerful algorithms, the unexpected connections between ideas from different fields, and more.</p>

<p>Likewise, “making mistakes” gets its own bad rap. I propose replacing it with “speeding up” because making more mistakes allows you to learn quicker. Whenever you are stuck and can’t make any progress because you are afraid of making mistakes, try to tell yourself “I am just going to speed up my learning a little.”</p>

<p>Making mistakes can be embarrassing, but <a href="https://twitter.com/edlatimore/status/1058305553020141570">embarrassment is simply the cost of entry</a>. The embarrassment you feel only exist in your mind, learning is what can actually happen. Once you allow yourself to look stupid and take on the beginner’s mindset, learning will be unstoppable.</p>

<p>Make more mistakes. It is okay to speed up a little, and it feels great.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Low-rank and Rank Structured Matrices]]></title>
    <link href="http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices/"/>
    <updated>2021-03-21T15:33:24-05:00</updated>
    <id>http://bobbielf2.github.io/blog/2021/03/21/low-rank-and-rank-structured-matrices</id>
    <content type="html"><![CDATA[<p>When you begin to research a field of study, you often get overwhelmed by the amount of existing knowledge you have to learn before you could go further. One useful way to bootstrap yourself is to only learn a minimal amount of basic ideas that are enough for you to “survive”  in the field. Such basic ideas are the Minimal Actionable Knowledge &amp; Experience (MAKE) of the field. Here, I will try to present the “MAKE” of the field of fast matrix computations.</p>

<!--more-->

<h3 id="low-rank-matrices-and-important-information">Low-rank matrices and important information</h3>

<p>An $m\times n$  matrix $\mathbf{A}$ is low-rank if its rank, $k\equiv\mathrm{rank}\,\mathbf{A}$, is far less than $m$ and $n$. Then $\mathbf{A}$ has a factorization <script type="math/tex">\mathbf{A} =\mathbf{E}\mathbf{F}</script> where $\mathbf{E}$ is a tall-skinny matrix with $k$ columns and $\mathbf{F}$ a short-fat matrix with $k$ rows.</p>

<p><img class="center" src="/images/blog_figures/lowRankFac.png" width="500"></p>

<p>For example the following $3\times3$ matrix is of rank-$1$ only.</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{bmatrix}
1 & 2 & 3\\
1 & 2 & 3\\
1 & 2 & 3
\end{bmatrix}
=
\begin{bmatrix}
1\\
1\\
1
\end{bmatrix}
\begin{bmatrix}
1 & 2 & 3
\end{bmatrix} %]]&gt;</script>

<p>Given a matrix $\mathbf{A}$, there are many ways to find $\mathrm{rank}\,\mathbf{A}$. One way is to find the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a></p>

<script type="math/tex; mode=display">\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*</script>

<p>where $\mathbf{\Sigma}=\mathrm{diag}(\sigma_1,\sigma_2,\dots)$ is an $m\times n$ diagonal matrix, whose diagonal elements are called the singular values of $\mathbf{A}$. Then $\mathrm{rank}\,\mathbf{A}$ is the number of nonzero singular values.</p>

<p>The SVD tells you the most important information about a matrix: the <a href="https://en.wikipedia.org/wiki/Low-rank_approximation#Proof_of_Eckart%E2%80%93Young%E2%80%93Mirsky_theorem_(for_spectral_norm)">Eckart-Young theorem</a> says that the best rank-$k$ approximation of $\mathbf{A}=\mathbf{U}\mathbf{\Sigma} \mathbf{V}^*$ can be obtained by only keeping the first $k$ singular values and zeroing out the rest in $\mathbf{\Sigma}$. When the singular values decay quickly, such a low-rank approximation can be very accurate. This is particularly important in practice when we want to solve problems efficiently by ignoring the unimportant information.</p>

<p>An interesting example is the $n\times n$ <a href="https://en.wikipedia.org/wiki/Hilbert_matrix">Hilbert matrix</a> $\mathbf{H}_n$, whose $(i,j)$ entry is defined to be $\frac{1}{i+j-1}$. $\mathbf{H}_n$ is full-rank for any size $n$, but it is <em>numerically low-rank</em>, meaning that its singular values decay rapidly such that given any small threshold $\epsilon$, only a few singular values are above $\epsilon$. For example with $\epsilon=10^{-15}$, the $1000\times1000$ has numerical rank $28$.</p>

<p>Other examples of (numerically) low-rank matrices include the <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix">Vandermonde</a>, <a href="https://en.wikipedia.org/wiki/Cauchy_matrix">Cauchy</a>, <a href="https://en.wikipedia.org/wiki/Hankel_matrix">Hankel</a>, and <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix">Toeplitz</a> matrices, as well as matrices constructed from smooth data or smooth functions.</p>

<p>As it turns out, a lot of the matrices we encounter in practice are numerically low-rank. So finding low-rank approximation (e.g. in the form of $\mathbf{A}=\mathbf{EF}$ at the beginning) is one of the most important and fundamental subjects in applied math nowadays.</p>

<h3 id="data-sparsity-and-rank-structured-matrices">Data sparsity and rank structured matrices</h3>

<p>Matrix sizes have been growing with technological advancements. Many common matrix algorithms scale cubically with the matrix size, meaning that even if your computing power grows 1000 times, you could only afford to solve problems that are 10 times bigger than before. These common algorithms include matrix multiplication, matrix inversion, and matrix factorizations (e.g. LU, QR, SVD). Therefore, it is important to speed up these matrix computation methods in order to fully exploit the ever growing computing power.</p>

<p>One major strategy to accelerate the computations is to exploit the <strong>data sparsity</strong> of a matrix. Data sparsity is a deliberately vague concept which broadly refers to the kind of internal structures in a matrix that can help make computations faster. Following are some common examples of data-sparse matrices.</p>

<ul>
  <li>The most classcial data-sparse matrices are the <strong>sparse matrices</strong>, ones with a large number of zero entries. Using <a href="https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)">compressed data formats</a>, you can save a lot of memories by storing only the nonzero entries (together with their positions). You can greatly reduce computation time by only operating on the nonzero entries while maintaining the sparsity of the matrices (i.e., avoid introducing more nonzero entries). Common sparse matrices include diagonal/block-diagonal matrices, banded matrices, permutations, adjacency matrix of graphs, etc.</li>
  <li><strong>Low-rank matrices</strong>, as we have introduced above, are ones admit low-rank factorizations. Commonly used factorizations include the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Reduced_SVDs">reduced SVDs</a>, <a href="https://en.wikipedia.org/wiki/Interpolative_decomposition">interpolative decomposition</a>, <a href="https://en.wikipedia.org/wiki/CUR_matrix_approximation">CUR decomposition</a>, <a href="https://en.wikipedia.org/wiki/RRQR_factorization">rank-revealing QR factorizations</a>, etc. Normally the SVD algorithms are more expensive, so the other algorithms are preferred when possible; for very large matrices, all the factorization algorithms can be further accelerated by <a href="https://epubs.siam.org/doi/abs/10.1137/090771806">randomization techniques</a>.</li>
  <li><strong>Rank structured matrices.</strong> These matrices are not necessarily low-rank, but can be split into a relatively small number of submatrices, each of which is low-rank. For example, the picture below shows the structure of a <em>Hierarchically Off-diagonal Low Rank</em> (HODLR) matrix, where all the off-diagonal blocks, big or small, have similar ranks. Such structure can for example arise from gravitational or electrostatic interactions, where the diagonal blocks represent the local interactions and the off-diagonal blocks represent the far interactions; the far interactions are low-rank because they are much smoother than the local interactions. Other rank structured matrices include the <em>Hierarchically Semi-separable</em> (HSS) matrices, the inverse of banded matrices, and the more general <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix">$\mathcal{H}$-matrices</a> and <a href="https://en.wikipedia.org/wiki/Hierarchical_matrix#H2-matrices">$\mathcal{H}^2$-matrices</a>. Rank structured matrices can be efficiently handled using tree structures. Matrix algorithms designed for these matrices can be very fast, with computation time scaling linearly or log-linearly with the matrix size.</li>
</ul>

<p><img class="center" src="/images/blog_figures/HODLR.png" width="400"></p>

<ul>
  <li><strong>Complementary low-rank matrices</strong> are a special type of rank structured matrices that can be decomposed by the <em>butterfly factorization</em> (BF). The BF is inspired by ideas of the <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> algorithm (divide-and-conquer and permutations), which can be explained using the <a href="https://en.wikipedia.org/wiki/Butterfly_diagram">butterfly diagram</a>. Butterfly algorithms were initially motivated by solving oscillatory problems such as wave scattering.</li>
</ul>

<p>With these ideas above, plus a little coding experience with some simple rank structured matrices (a good place to start is with the first two of these <a href="https://amath.colorado.edu/faculty/martinss/2014_CBMS/codes.html">tutorial codes</a>), you are equipped with the “MAKE” that gets you ready for going on an advanture to the fast computations with matrices. All the details and other more advanced topics can be learned later once you dig far enough.</p>
]]></content>
  </entry>
  
</feed>
